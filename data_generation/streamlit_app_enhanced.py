import os
# ç¦ç”¨torchç¼–è¯‘å™¨å’Œinductoræ¥é¿å…CUDAå›¾å½¢é”™è¯¯
os.environ["TORCH_COMPILE_DISABLE"] = "1"
os.environ["TORCHINDUCTOR_DISABLE"] = "1"

import uuid
import shutil
import numpy as np
import cv2
import streamlit as st

st.set_page_config(layout="wide")
from PIL import Image
import torch
from moviepy import VideoFileClip
from torchvision.ops import masks_to_boxes
from sam2.build_sam import build_sam2_video_predictor
from streamlit_image_coordinates import streamlit_image_coordinates

# æ™ºèƒ½è®¾å¤‡æ£€æµ‹å‡½æ•°
def get_device():
    """æ™ºèƒ½æ£€æµ‹æœ€ä½³å¯ç”¨è®¾å¤‡ï¼šMPS > CUDA > CPU"""
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        return torch.device("mps")
    elif torch.cuda.is_available():
        return torch.device("cuda")
    else:
        return torch.device("cpu")

def get_autocast_device():
    """è·å–ç”¨äºautocastçš„è®¾å¤‡ç±»å‹å­—ç¬¦ä¸²"""
    device = get_device()
    if device.type == "mps":
        return "cpu"  # MPSåœ¨autocastä¸­ä½¿ç”¨cpuæ¨¡å¼
    else:
        return device.type

# è¾¹ç•Œæ¡†ç”Ÿæˆå‡½æ•°
def get_tight_bbox(mask):
    """
    åŸºäºåƒç´ åˆ†å¸ƒçš„ç´§å¯†è¾¹ç•Œæ¡†
    """
    if mask.sum() == 0:
        return None
    
    # ç¡®ä¿æ©ç æ˜¯2D
    if len(mask.shape) == 3:
        mask = mask.squeeze()
    if len(mask.shape) != 2:
        return None
    
    # æ‰¾åˆ°æ‰€æœ‰å‰æ™¯åƒç´ çš„ä½ç½®
    y_indices, x_indices = np.where(mask > 0)
    
    if len(y_indices) == 0:
        return None
    
    # è®¡ç®—è¾¹ç•Œæ¡†
    x_min, x_max = x_indices.min(), x_indices.max()
    y_min, y_max = y_indices.min(), y_indices.max()
    
    return [x_min, y_min, x_max + 1, y_max + 1]

def get_contour_bbox(mask):
    """
    åŸºäºè½®å»“çš„æ›´ç²¾ç¡®è¾¹ç•Œæ¡†
    """
    if mask.sum() == 0:
        return None
    
    # ç¡®ä¿æ©ç æ˜¯2Dä¸”ä¸ºuint8ç±»å‹
    if len(mask.shape) == 3:
        mask = mask.squeeze()
    if len(mask.shape) != 2:
        return None
    
    # ç¡®ä¿æ©ç æ˜¯äºŒå€¼çš„uint8ç±»å‹
    mask_uint8 = (mask > 0).astype(np.uint8)
    
    # æ‰¾åˆ°è½®å»“
    contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    if not contours:
        return None
    
    # æ‰¾åˆ°æœ€å¤§çš„è½®å»“
    largest_contour = max(contours, key=cv2.contourArea)
    
    # è·å–è½®å»“çš„è¾¹ç•Œæ¡†
    x, y, w, h = cv2.boundingRect(largest_contour)
    
    return [x, y, x + w, y + h]

def get_bbox_by_method(mask, method="è½®å»“è¾¹ç•Œæ¡†"):
    """
    æ ¹æ®é€‰æ‹©çš„æ–¹æ³•ç”Ÿæˆè¾¹ç•Œæ¡†
    """
    if mask.sum() == 0:
        return None
    
    # ç¡®ä¿æ©ç æ˜¯2D
    if len(mask.shape) == 3:
        mask = mask.squeeze()
    if len(mask.shape) != 2:
        return None
    
    if method == "è½®å»“è¾¹ç•Œæ¡†":
        box = get_contour_bbox(mask)
        if box is None:
            box = get_tight_bbox(mask)
        return box
    elif method == "ç´§å¯†è¾¹ç•Œæ¡†":
        return get_tight_bbox(mask)
    elif method == "ä¼ ç»Ÿæ–¹æ³•":
        try:
            # ç¡®ä¿æ©ç æ˜¯2Dä¸”ä¸ºå¸ƒå°”ç±»å‹
            mask_2d = (mask > 0).astype(bool)
            box = masks_to_boxes(torch.tensor(mask_2d[None]))[0].int().tolist()
            return box
        except:
            return get_tight_bbox(mask)
    else:
        return get_contour_bbox(mask)

# åˆå§‹åŒ–æ¨¡å‹
@st.cache_resource
def load_sam2_model():
    checkpoint = os.path.join(os.path.expanduser("~"), "mysam", "sam2", "checkpoints", "sam2.1_hiera_base_plus.pt")
    model_cfg = "configs/sam2.1/sam2.1_hiera_b+.yaml"
    
    # æ™ºèƒ½è®¾å¤‡æ£€æµ‹ï¼šä¼˜å…ˆä½¿ç”¨MPSåŠ é€Ÿ (Apple Silicon)ï¼Œç„¶åCUDAï¼Œæœ€åCPU
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        device = torch.device("mps")
        print("ğŸš€ ä½¿ç”¨Apple Silicon MPSåŠ é€Ÿ")
        st.success("ğŸš€ æ­£åœ¨ä½¿ç”¨Apple Silicon MPSåŠ é€Ÿ")
    elif torch.cuda.is_available():
        device = torch.device("cuda")
        print("ğŸš€ ä½¿ç”¨CUDA GPUåŠ é€Ÿ")
        st.success("ğŸš€ æ­£åœ¨ä½¿ç”¨CUDA GPUåŠ é€Ÿ")
    else:
        device = torch.device("cpu")
        print("ğŸ’» ä½¿ç”¨CPUå¤„ç†")
        st.info("ğŸ’» æ­£åœ¨ä½¿ç”¨CPUå¤„ç†")
    
    return build_sam2_video_predictor(model_cfg, checkpoint, device=device, vos_optimized=True)

sam2_model = load_sam2_model()

st.title("ğŸ¬ SAM2 æ™ºèƒ½è§†é¢‘æ ‡æ³¨å·¥å…· & YOLO11æ ¼å¼å¯¼å‡º")

# ä¾§è¾¹æ ï¼šæ•°æ®é›†ç®¡ç†
with st.sidebar:
    st.header("ğŸ“ æ•°æ®é›†ç®¡ç†")
    
    # æ•°æ®é›†åç§°è®¾ç½®
    dataset_name = st.text_input(
        "æ•°æ®é›†åç§°", 
        value=st.session_state.get("dataset_name", "lajiao_dataset"),
        help="æ‰€æœ‰è§†é¢‘çš„æ ‡æ³¨æ•°æ®å°†ç»Ÿä¸€ä¿å­˜åˆ°è¿™ä¸ªæ•°æ®é›†ä¸­"
    )
    st.session_state["dataset_name"] = dataset_name
    
    # æ˜¾ç¤ºå½“å‰æ•°æ®é›†ç»Ÿè®¡
    unified_frame_dir = f"frames_{dataset_name}"
    unified_label_dir = f"labels_{dataset_name}"
    
    if os.path.exists(unified_frame_dir):
        frame_count = len([f for f in os.listdir(unified_frame_dir) if f.endswith('.jpg')])
        st.metric("ğŸ“· æ€»å›¾åƒæ•°", frame_count)
    else:
        st.metric("ğŸ“· æ€»å›¾åƒæ•°", 0)
    
    if os.path.exists(unified_label_dir):
        label_count = len([f for f in os.listdir(unified_label_dir) if f.endswith('.txt')])
        st.metric("ğŸ·ï¸ æ€»æ ‡æ³¨æ•°", label_count)
    else:
        st.metric("ğŸ·ï¸ æ€»æ ‡æ³¨æ•°", 0)
    
    # æ•°æ®é›†æ“ä½œæŒ‰é’®
    if st.button("ğŸ”„ åˆ·æ–°ç»Ÿè®¡"):
        st.rerun()
    
    if st.button("ğŸ“¦ å¯¼å‡ºå½“å‰æ•°æ®é›†"):
        if os.path.exists(unified_frame_dir) and os.path.exists(unified_label_dir):
            import zipfile
            zip_path = f"{dataset_name}_export.zip"
            with zipfile.ZipFile(zip_path, 'w') as zipf:
                # æ·»åŠ å›¾åƒæ–‡ä»¶
                for file in os.listdir(unified_frame_dir):
                    if file.endswith('.jpg'):
                        zipf.write(os.path.join(unified_frame_dir, file), f"images/{file}")
                # æ·»åŠ æ ‡æ³¨æ–‡ä»¶
                for file in os.listdir(unified_label_dir):
                    if file.endswith('.txt'):
                        zipf.write(os.path.join(unified_label_dir, file), f"labels/{file}")
            st.success(f"âœ… æ•°æ®é›†å·²å¯¼å‡ºä¸º: {zip_path}")
        else:
            st.warning("âš ï¸ æ•°æ®é›†ä¸ºç©ºï¼Œæ— æ³•å¯¼å‡º")
    
    st.divider()
    
    # è¾¹ç•Œæ¡†ç®—æ³•é€‰æ‹©
    st.header("âš™ï¸ ç®—æ³•é…ç½®")
    bbox_method = st.selectbox(
        "è¾¹ç•Œæ¡†ç®—æ³•",
        ["ä¼ ç»Ÿæ–¹æ³•", "è½®å»“è¾¹ç•Œæ¡†", "ç´§å¯†è¾¹ç•Œæ¡†"],
        help="ä¼ ç»Ÿæ–¹æ³•ï¼šæœ€å°å¤–æ¥çŸ©å½¢ï¼Œæ•ˆæœæœ€å¥½\\nè½®å»“è¾¹ç•Œæ¡†ï¼šåŸºäºç‰©ä½“è½®å»“\\nç´§å¯†è¾¹ç•Œæ¡†ï¼šåŸºäºåƒç´ åˆ†å¸ƒ"
    )
    st.session_state["bbox_method"] = bbox_method

VIDEO_DIR = "video_segments"
os.makedirs(VIDEO_DIR, exist_ok=True)

# ä¸Šä¼ è§†é¢‘
uploaded_video = st.file_uploader("ğŸ“ ä¸Šä¼ å®Œæ•´åŸå§‹è§†é¢‘", type=["mp4"])
if uploaded_video:
    original_path = "temp_uploaded.mp4"
    with open(original_path, "wb") as f:
        f.write(uploaded_video.read())

    cap = cv2.VideoCapture(original_path)
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    
    # é˜²æ­¢é™¤é›¶é”™è¯¯
    if fps == 0:
        fps = 30  # é»˜è®¤å¸§ç‡
        st.warning("âš ï¸ æ— æ³•è·å–è§†é¢‘å¸§ç‡ï¼Œä½¿ç”¨é»˜è®¤å€¼30fps")
    
    duration = frame_count // fps
    st.video(original_path)

    st.subheader("âœ‚ï¸ è§†é¢‘è£å‰ª")
    start_time = st.slider("èµ·å§‹æ—¶é—´ï¼ˆç§’ï¼‰", 0, duration - 1, 0)
    end_time = st.slider("ç»“æŸæ—¶é—´ï¼ˆç§’ï¼‰", start_time + 1, duration, start_time + 5)

    if st.button("è£å‰ªå¹¶ä¿å­˜ç‰‡æ®µ"):
        clip = VideoFileClip(original_path).subclipped(start_time, end_time)
        
        # ä½¿ç”¨ç»Ÿä¸€çš„æ•°æ®é›†åç§°ï¼Œè€Œä¸æ˜¯éšæœºsession_id
        dataset_name = st.session_state.get("dataset_name", "lajiao_dataset")
        session_id = uuid.uuid4().hex[:8]  # åªç”¨äºè§†é¢‘æ–‡ä»¶å‘½å
        segment_path = os.path.join(VIDEO_DIR, f"{session_id}.mp4")
        clip.write_videofile(segment_path, codec="libx264")
        st.success(f"âœ… è§†é¢‘è£å‰ªå®Œæˆ: {segment_path}")
        st.session_state["segment_path"] = segment_path
        st.session_state["current_session_id"] = session_id  # å½“å‰è§†é¢‘çš„ID
        cap.release()
        shutil.move(original_path, f"{original_path}.bak")

# è§†é¢‘æ‹†å¸§ + åŠ è½½çŠ¶æ€
current_session_id = st.session_state.get("current_session_id", None)
segment_path = st.session_state.get("segment_path", None)
dataset_name = st.session_state.get("dataset_name", "lajiao_dataset")

# ä½¿ç”¨ç»Ÿä¸€çš„ç›®å½•åç§°
UNIFIED_FRAME_DIR = f"frames_{dataset_name}"
UNIFIED_LABEL_DIR = f"labels_{dataset_name}"

if current_session_id and segment_path:
    # åˆ›å»ºç»Ÿä¸€çš„æ•°æ®ç›®å½•
    os.makedirs(UNIFIED_FRAME_DIR, exist_ok=True)
    os.makedirs(UNIFIED_LABEL_DIR, exist_ok=True)
    
    # ä¸ºå½“å‰è§†é¢‘åˆ›å»ºä¸´æ—¶å¸§ç›®å½•
    FRAME_DIR = f"frame_cache_{current_session_id}"
    os.makedirs(FRAME_DIR, exist_ok=True)

    if not os.listdir(FRAME_DIR):
        cap = cv2.VideoCapture(segment_path)
        frame_idx = 0
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            frame_path = os.path.join(FRAME_DIR, f"{frame_idx:04d}.jpg")
            cv2.imwrite(frame_path, frame)
            frame_idx += 1
        cap.release()
        st.success(f"âœ… å…±æå– {frame_idx} å¸§è‡³ç¼“å­˜ç›®å½• {FRAME_DIR}")

    frame_files = sorted(os.listdir(FRAME_DIR))
    
    # æ·»åŠ å·¥ä½œæ¨¡å¼é€‰æ‹©
    st.subheader("ğŸ”§ å·¥ä½œæ¨¡å¼é€‰æ‹©")
    work_mode = st.radio("é€‰æ‹©å·¥ä½œæ¨¡å¼", ["åˆå§‹æ ‡æ³¨", "é¢„è§ˆ100å¸§æ©ç ", "ä¿®æ­£ç‰¹å®šå¸§"], horizontal=True)
    
    if work_mode == "åˆå§‹æ ‡æ³¨":
        frame_index = st.session_state.get("frame_index", 0)
        current_frame_path = os.path.join(FRAME_DIR, frame_files[frame_index])
        current_img = Image.open(current_frame_path)

        frame_np = np.array(current_img.convert("RGB"))
        preview_img = frame_np.copy()

        points = st.session_state.get("points", {}).get(frame_index, [])
        for x, y, l in points:
            color = (0, 255, 0) if l == 1 else (0, 0, 255)
            cv2.circle(preview_img, (int(x), int(y)), 5, color, -1)

        # å·¦å›¾å³æ§
        col1, col2 = st.columns([3, 1])

        with col1:
            # é¢„è§ˆå¸§å¹¶ç‚¹å‡»æ‰“ç‚¹
            click = streamlit_image_coordinates(preview_img, key=f"frame_{frame_index}_{st.session_state.get('refresh_flag', False)}")
            if click:
                if "points" not in st.session_state:
                    st.session_state["points"] = {}
                if frame_index not in st.session_state["points"]:
                    st.session_state["points"][frame_index] = []
                # è®°å½•å½“å‰ç‚¹å‡»åæ ‡ï¼Œç­‰å¾…ç”¨æˆ·é€‰æ‹©æ­£/è´Ÿç‚¹
                st.session_state["last_click"] = (click["x"], click["y"])

        with col2:
            st.markdown("### ğŸï¸ å½“å‰å¸§æ§åˆ¶")
            frame_index = st.slider("å¸§ä½ç½®", 0, len(frame_files) - 1, value=frame_index, key="frame_index")
            st.write(f"å½“å‰å¸§ç¼–å·ï¼š**{frame_index}**")

            # æ ‡ç­¾ç®¡ç†
            st.subheader("ğŸ·ï¸ æ ‡ç­¾è¾“å…¥ä¸ç¡®è®¤")
            if "label_history" not in st.session_state:
                st.session_state["label_history"] = []
            label_input = st.text_input("âœï¸ è¾“å…¥æ ‡ç­¾å", value="", placeholder="e.g. çŒªè‚‰")
            if label_input:
                suggestions = [l for l in st.session_state["label_history"] if l.startswith(label_input.lower())]
                if suggestions:
                    st.markdown("ğŸ” è‡ªåŠ¨è¡¥å…¨å»ºè®®ï¼š" + ", ".join(suggestions[:5]))

            if st.button("âœ… ç¡®å®šæ ‡ç­¾"):
                label = label_input.strip().lower()
                if label:
                    if label not in st.session_state["label_history"]:
                        st.session_state["label_history"].append(label)
                    st.session_state["current_label"] = label
                    st.success(f"âœ… å½“å‰ä½¿ç”¨æ ‡ç­¾ï¼š`{label}`")
                else:
                    st.warning("âš ï¸ æ ‡ç­¾ä¸èƒ½ä¸ºç©º")

            label = st.session_state.get("current_label", None)

            # æ–°å¢ï¼šä¿ç•™ç‚¹/å»é™¤ç‚¹æŒ‰é’®
            if "last_click" in st.session_state:
                col_a, col_b = st.columns(2)
                with col_a:
                    if st.button("ä¿ç•™ç‚¹(æ­£ç‚¹)"):
                        if frame_index not in st.session_state["points"]:
                            st.session_state["points"][frame_index] = []
                        st.session_state["points"][frame_index].append((*st.session_state["last_click"], 1))
                        st.session_state["refresh_flag"] = not st.session_state.get("refresh_flag", False)
                        del st.session_state["last_click"]
                        st.rerun()
                with col_b:
                    if st.button("å»é™¤ç‚¹(è´Ÿç‚¹)"):
                        if frame_index not in st.session_state["points"]:
                            st.session_state["points"][frame_index] = []
                        st.session_state["points"][frame_index].append((*st.session_state["last_click"], 0))
                        st.session_state["refresh_flag"] = not st.session_state.get("refresh_flag", False)
                        del st.session_state["last_click"]
                        st.rerun()

            if st.button("ğŸ§¹ æ¸…é™¤å½“å‰å¸§æ‰€æœ‰ç‚¹"):
                if "points" in st.session_state and frame_index in st.session_state["points"]:
                    st.session_state["points"][frame_index] = []
                st.success("âœ… å½“å‰å¸§ç‚¹æ¸…é™¤å®Œæ¯•")
                st.session_state["refresh_flag"] = not st.session_state.get("refresh_flag", False)

            if st.button("ğŸ§¼ æ¸…é™¤æ‰€æœ‰å¸§çš„ç‚¹"):
                st.session_state["points"] = {}
                st.success("âœ… æ‰€æœ‰å¸§ç‚¹æ¸…é™¤å®Œæ¯•")
                st.session_state["refresh_flag"] = not st.session_state.get("refresh_flag", False)

            if st.button("ğŸ‘ï¸ é¢„è§ˆå½“å‰å¸§æ ‡æ³¨"):
                if not points:
                    st.warning("âš ï¸ å½“å‰å¸§æ— ç‚¹å‡»ç‚¹")
                else:
                    pts = []
                    lbls = []
                    for x, y, l in points:
                        pts.append([x, y])
                        lbls.append(l)
                    
                    # æ·»åŠ è°ƒè¯•ä¿¡æ¯
                    st.write(f"ğŸ” è°ƒè¯•ä¿¡æ¯ï¼šç‚¹å‡»ç‚¹æ•°é‡: {len(points)}")
                    st.write(f"ğŸ” ç‚¹åæ ‡: {pts}")
                    st.write(f"ğŸ” ç‚¹æ ‡ç­¾: {lbls}")
                    
                    try:
                        # æ¸…ç†GPU/MPSå†…å­˜
                        device = get_device()
                        if device.type == "cuda":
                            torch.cuda.empty_cache()
                        elif device.type == "mps":
                            torch.mps.empty_cache()
                        
                        with torch.autocast(get_autocast_device()):
                            # é™åˆ¶è§†é¢‘é•¿åº¦ä¸º100å¸§æ¥èŠ‚çœå†…å­˜
                            temp_video_path = f"temp_video_100frames_{current_session_id}.mp4"
                            if not os.path.exists(temp_video_path):
                                # åˆ›å»ºåªæœ‰å‰100å¸§çš„ä¸´æ—¶è§†é¢‘
                                cap = cv2.VideoCapture(segment_path)
                                fps = int(cap.get(cv2.CAP_PROP_FPS))
                                width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                                height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                                
                                fourcc = cv2.VideoWriter_fourcc(*'mp4v')
                                out = cv2.VideoWriter(temp_video_path, fourcc, fps, (width, height))
                                
                                frame_count = 0
                                while frame_count < 100:
                                    ret, frame = cap.read()
                                    if not ret:
                                        break
                                    out.write(frame)
                                    frame_count += 1
                                
                                cap.release()
                                out.release()
                                st.write(f"âœ… åˆ›å»º100å¸§ä¸´æ—¶è§†é¢‘: {frame_count} å¸§")
                            
                            # ä½¿ç”¨å¸§ç›®å½•è€Œä¸æ˜¯è§†é¢‘æ–‡ä»¶ï¼Œå°±åƒGPU.pyä¸­ä¸€æ ·
                            inference_state = sam2_model.init_state(video_path=FRAME_DIR)
                            st.write("âœ… æ¨ç†çŠ¶æ€åˆå§‹åŒ–æˆåŠŸ")
                            
                            _, _, mask_logits = sam2_model.add_new_points_or_box(
                                inference_state=inference_state,
                                frame_idx=frame_index,
                                obj_id=1,  # ä½¿ç”¨obj_id=1ï¼Œå°±åƒGPU.pyä¸­ä¸€æ ·
                                points=np.array(pts, dtype=np.float32),
                                labels=np.array(lbls, dtype=np.int32),
                            )
                            st.write(f"âœ… SAM2é¢„æµ‹å®Œæˆï¼Œæ©ç å½¢çŠ¶: {mask_logits.shape}")
                            
                            # ç›´æ¥ä½¿ç”¨GPU.pyä¸­çš„é€»è¾‘ï¼š(mask_logits[0] > 0)
                            binary_mask = (mask_logits[0] > 0).cpu().numpy().squeeze()
                            st.write(f"ğŸ” æ©ç ç»Ÿè®¡: å½¢çŠ¶={binary_mask.shape}, å‰æ™¯åƒç´ ={binary_mask.sum()}")
                            
                            # ç«‹å³æ¸…ç†GPU/MPSå†…å­˜
                            del mask_logits
                            if device.type == "cuda":
                                torch.cuda.empty_cache()
                            elif device.type == "mps":
                                torch.mps.empty_cache()
                            
                            if binary_mask.sum() > 0:
                                # ä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„è¾¹ç•Œæ¡†ç®—æ³•ï¼Œä¼ ç»Ÿæ–¹æ³•æ•ˆæœæœ€å¥½
                                bbox_method = st.session_state.get("bbox_method", "ä¼ ç»Ÿæ–¹æ³•")
                                box = get_bbox_by_method(binary_mask, bbox_method)
                                
                                if box is not None:
                                    x1, y1, x2, y2 = box
                                    overlay = frame_np.copy()
                                    overlay[binary_mask == 1] = (overlay[binary_mask == 1] * 0.5 + np.array([128, 128, 255]) * 0.5).astype(np.uint8)
                                    for x, y, l in points:
                                        cv2.circle(overlay, (int(x), int(y)), 5, (0,255,0) if l==1 else (0,0,255), -1)
                                    cv2.rectangle(overlay, (x1, y1), (x2, y2), (0,255,0), 2)
                                    st.image(overlay, caption=f"æ ‡ç­¾: {label} | ç‚¹æ•°: {len(points)} | BBox: {box} | æ–¹æ³•: {bbox_method}")
                                    
                                    if label is not None and label in st.session_state["label_history"]:
                                        # ä¿å­˜åˆ°ç»Ÿä¸€çš„æ•°æ®é›†ç›®å½•
                                        label_id = st.session_state["label_history"].index(label)
                                        h, w = binary_mask.shape
                                        yolo_line = f"{label_id} {(x1+x2)/2/w:.6f} {(y1+y2)/2/h:.6f} {(x2-x1)/w:.6f} {(y2-y1)/h:.6f}\n"
                                        
                                        # ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶åï¼ˆé¿å…é‡å¤ï¼‰
                                        base_name = frame_files[frame_index].replace(".jpg", "")
                                        unique_name = f"{current_session_id}_{base_name}"
                                        
                                        # ä¿å­˜å›¾åƒåˆ°ç»Ÿä¸€ç›®å½•
                                        src_img_path = os.path.join(FRAME_DIR, frame_files[frame_index])
                                        dst_img_path = os.path.join(UNIFIED_FRAME_DIR, f"{unique_name}.jpg")
                                        shutil.copy2(src_img_path, dst_img_path)
                                        
                                        # ä¿å­˜æ ‡æ³¨åˆ°ç»Ÿä¸€ç›®å½•
                                        label_file = os.path.join(UNIFIED_LABEL_DIR, f"{unique_name}.txt")
                                        with open(label_file, "w") as f:
                                            f.write(yolo_line)
                                        
                                        # æ›´æ–°ç±»åˆ«æ–‡ä»¶
                                        classes_file = os.path.join(UNIFIED_LABEL_DIR, "classes.txt")
                                        with open(classes_file, "w") as f:
                                            for label_name in st.session_state["label_history"]:
                                                f.write(f"{label_name}\n")
                                        
                                        st.success(f"âœ… å•å¸§æ•°æ®å·²æ·»åŠ åˆ°æ•°æ®é›†: {unique_name}")
                                else:
                                    st.warning("âš ï¸ æ— æ³•ç”Ÿæˆæœ‰æ•ˆè¾¹ç•Œæ¡†")
                            else:
                                st.warning("âš ï¸ æ©ç ä¸ºç©ºï¼Œå¯èƒ½æ˜¯ç‚¹å‡»ä½ç½®ä¸åˆé€‚æˆ–æ¨¡å‹é¢„æµ‹å¤±è´¥")
                                st.write("ğŸ’¡ å»ºè®®ï¼šå°è¯•ç‚¹å‡»ç›®æ ‡ç‰©ä½“çš„ä¸­å¿ƒåŒºåŸŸï¼Œæˆ–æ·»åŠ æ›´å¤šæ­£ç‚¹")
                    
                    except Exception as e:
                        st.error(f"âŒ é¢„æµ‹è¿‡ç¨‹å‡ºé”™: {str(e)}")
                        import traceback
                        st.code(traceback.format_exc())

            if st.button("âš¡ ç”Ÿæˆ100å¸§æ©ç "):
                ref_points = st.session_state["points"].get(frame_index, [])
                if not ref_points or not label:
                    st.warning("âš ï¸ å½“å‰å¸§æœªæ‰“ç‚¹æˆ–æ ‡ç­¾æœªè®¾ç½®")
                else:
                    with st.spinner("æ­£åœ¨ç”Ÿæˆ100å¸§æ©ç ..."):
                        try:
                            # æ¸…ç†GPU/MPSå†…å­˜
                            device = get_device()
                            if device.type == "cuda":
                                torch.cuda.empty_cache()
                            elif device.type == "mps":
                                torch.mps.empty_cache()
                                
                            with torch.autocast(get_autocast_device()):
                                # ä½¿ç”¨å¸§ç›®å½•ï¼Œå°±åƒGPU.pyä¸­ä¸€æ ·
                                inference_state = sam2_model.init_state(video_path=FRAME_DIR)
                                pts = [[p[0], p[1]] for p in ref_points]
                                lbls = [p[2] for p in ref_points]
                                
                                st.write(f"ğŸ” ä½¿ç”¨å¸§ {frame_index} ä½œä¸ºå‚è€ƒå¸§")
                                st.write(f"ğŸ” å‚è€ƒç‚¹: {pts}, æ ‡ç­¾: {lbls}")
                                
                                # ä½¿ç”¨GPU.pyä¸­çš„é€»è¾‘
                                _, _, _ = sam2_model.add_new_points_or_box(
                                    inference_state=inference_state,
                                    frame_idx=frame_index,
                                    obj_id=1,  # ä½¿ç”¨obj_id=1
                                    points=np.array(pts, dtype=np.float32),
                                    labels=np.array(lbls, dtype=np.int32),
                                )
                                
                                # æ‰¹é‡ä¼ æ’­ï¼Œä½¿ç”¨GPU.pyä¸­çš„é€»è¾‘
                                video_segments = {}
                                for i, obj_ids, mask_logits in sam2_model.propagate_in_video(inference_state):
                                    video_segments[i] = {
                                        obj_id: (mask_logits[j] > 0).cpu().numpy()
                                        for j, obj_id in enumerate(obj_ids)
                                    }
                                    # ç«‹å³æ¸…ç†æ¯å¸§çš„GPU/MPSå†…å­˜
                                    if device.type == "cuda":
                                        torch.cuda.empty_cache()
                                    elif device.type == "mps":
                                        torch.mps.empty_cache()
                                    # é™åˆ¶åªå¤„ç†å‰100å¸§
                                    if i >= 99:
                                        break
                                
                                # ç»Ÿè®¡ç”Ÿæˆçš„æ©ç 
                                valid_masks = sum(1 for frame_data in video_segments.values() 
                                                for mask in frame_data.values() if mask.sum() > 0)
                                st.write(f"âœ… æˆåŠŸç”Ÿæˆ {valid_masks} ä¸ªæœ‰æ•ˆæ©ç ")
                                
                                st.session_state["video_segments"] = video_segments
                                st.session_state["inference_state"] = inference_state
                                st.session_state["reference_frame"] = frame_index
                        
                        except Exception as e:
                            st.error(f"âŒ ç”Ÿæˆ100å¸§æ©ç æ—¶å‡ºé”™: {str(e)}")
                            import traceback
                            st.code(traceback.format_exc())
                    
                    st.success("âœ… 100å¸§æ©ç ç”Ÿæˆå®Œæˆï¼Œè¯·åˆ‡æ¢åˆ°'é¢„è§ˆ100å¸§æ©ç 'æ¨¡å¼æŸ¥çœ‹")

    elif work_mode == "é¢„è§ˆ100å¸§æ©ç ":
        if "video_segments" not in st.session_state:
            st.warning("âš ï¸ è¯·å…ˆåœ¨'åˆå§‹æ ‡æ³¨'æ¨¡å¼ä¸‹ç”Ÿæˆ100å¸§æ©ç ")
        else:
            st.subheader("ğŸï¸ 100å¸§æ©ç é¢„è§ˆ")
            max_frames = min(len(frame_files), 100)
            preview_frame_idx = st.slider("é¢„è§ˆå¸§ä½ç½®", 0, max_frames-1, 0, key="preview_frame_idx")
            
            col1, col2 = st.columns([3, 1])
            
            with col1:
                # ç¡®ä¿æ¯æ¬¡éƒ½é‡æ–°åŠ è½½å¸§
                frame_path = os.path.join(FRAME_DIR, frame_files[preview_frame_idx])
                img = np.array(Image.open(frame_path).convert("RGB"))
                
                video_segments = st.session_state["video_segments"]
                mask = video_segments.get(preview_frame_idx, {}).get(1, None)  # ä½¿ç”¨obj_id=1
                
                # æ·»åŠ è°ƒè¯•ä¿¡æ¯
                st.write(f"ğŸ” å½“å‰é¢„è§ˆå¸§: {preview_frame_idx}, å¸§æ–‡ä»¶: {frame_files[preview_frame_idx]}")
                st.write(f"ğŸ” æ©ç çŠ¶æ€: {'æœ‰æ©ç ' if mask is not None and mask.sum() > 0 else 'æ— æ©ç '}")
                
                if mask is not None and mask.sum() > 0:
                    overlay = img.copy()
                    
                    # ç¡®ä¿æ©ç å½¢çŠ¶ä¸å›¾åƒåŒ¹é…
                    mask_valid = True
                    if len(mask.shape) == 3:
                        mask = mask.squeeze()
                    if len(mask.shape) == 1:
                        # å¦‚æœæ©ç æ˜¯1Dï¼Œéœ€è¦é‡æ–°reshape
                        h, w = img.shape[:2]
                        if mask.size == h * w:
                            mask = mask.reshape(h, w)
                        else:
                            st.error(f"æ©ç å¤§å°ä¸åŒ¹é…: æ©ç å¤§å°={mask.size}, å›¾åƒå¤§å°={h}x{w}")
                            mask_valid = False
                    
                    if mask_valid and len(mask.shape) == 2:
                        # ç¡®ä¿æ©ç æ˜¯äºŒå€¼çš„
                        mask = (mask > 0).astype(np.uint8)
                        
                        # æ£€æŸ¥æ©ç å’Œå›¾åƒå½¢çŠ¶æ˜¯å¦åŒ¹é…
                        if mask.shape[:2] == img.shape[:2]:
                            overlay[mask == 1] = (overlay[mask == 1] * 0.5 + np.array([128,128,255]) * 0.5).astype(np.uint8)
                            
                            # ä½¿ç”¨æ›´ç²¾ç¡®çš„è¾¹ç•Œæ¡†ç®—æ³•ï¼Œä¼ ç»Ÿæ–¹æ³•æ•ˆæœæœ€å¥½
                            bbox_method = st.session_state.get("bbox_method", "ä¼ ç»Ÿæ–¹æ³•")
                            box = get_bbox_by_method(mask, bbox_method)
                            
                            if box is not None:
                                x1, y1, x2, y2 = box
                                cv2.rectangle(overlay, (x1, y1), (x2, y2), (0,255,0), 2)
                                st.image(overlay, caption=f"å¸§ {preview_frame_idx} - æ©ç é¢„è§ˆ | BBox: {box}")
                            else:
                                st.image(overlay, caption=f"å¸§ {preview_frame_idx} - æ©ç é¢„è§ˆ (æ— è¾¹ç•Œæ¡†)")
                        else:
                            st.error(f"æ©ç å½¢çŠ¶ {mask.shape} ä¸å›¾åƒå½¢çŠ¶ {img.shape[:2]} ä¸åŒ¹é…")
                            st.image(img, caption=f"å¸§ {preview_frame_idx} - æ©ç å½¢çŠ¶é”™è¯¯")
                    else:
                        st.image(img, caption=f"å¸§ {preview_frame_idx} - æ©ç æ— æ•ˆ")
                else:
                    st.image(img, caption=f"å¸§ {preview_frame_idx} - æ— æ©ç ")
            
            with col2:
                st.markdown("### ğŸ”§ æ“ä½œé€‰é¡¹")
                if st.button("é€‰æ‹©æ­¤å¸§è¿›è¡Œä¿®æ­£"):
                    st.session_state["refine_frame_idx"] = preview_frame_idx
                    st.success(f"âœ… å·²é€‰æ‹©å¸§ {preview_frame_idx} è¿›è¡Œä¿®æ­£")
                
                label = st.session_state.get("current_label", "unknown")
                if st.button("ğŸ“¤ æ·»åŠ åˆ°ç»Ÿä¸€æ•°æ®é›†"):
                    if label not in st.session_state.get("label_history", []):
                        st.warning("âš ï¸ è¯·å…ˆè®¾ç½®æœ‰æ•ˆæ ‡ç­¾")
                    else:
                        # æ‰¹é‡æ·»åŠ åˆ°ç»Ÿä¸€æ•°æ®é›†
                        exported_count = 0
                        for i in range(max_frames):
                            mask = video_segments.get(i, {}).get(1, None)  # ä½¿ç”¨obj_id=1
                            if mask is not None and mask.sum() > 0:
                                # ä½¿ç”¨æ›´ç²¾ç¡®çš„è¾¹ç•Œæ¡†ç®—æ³•ï¼Œä¼ ç»Ÿæ–¹æ³•æ•ˆæœæœ€å¥½
                                bbox_method = st.session_state.get("bbox_method", "ä¼ ç»Ÿæ–¹æ³•")
                                box = get_bbox_by_method(mask, bbox_method)
                                
                                if box is not None:
                                    x1, y1, x2, y2 = box
                                    # ç¡®ä¿æ©ç æ˜¯2Dçš„
                                    if len(mask.shape) == 3:
                                        mask = mask.squeeze()
                                    if len(mask.shape) == 2:
                                        h, w = mask.shape
                                    else:
                                        h, w = img.shape[:2]  # ä½¿ç”¨å›¾åƒå°ºå¯¸ä½œä¸ºåå¤‡
                                    
                                    label_id = st.session_state["label_history"].index(label)
                                    yolo_line = f"{label_id} {(x1+x2)/2/w:.6f} {(y1+y2)/2/h:.6f} {(x2-x1)/w:.6f} {(y2-y1)/h:.6f}\n"
                                    
                                    # ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶å
                                    base_name = frame_files[i].replace(".jpg", "")
                                    unique_name = f"{current_session_id}_{base_name}"
                                    
                                    # ä¿å­˜å›¾åƒåˆ°ç»Ÿä¸€ç›®å½•
                                    src_img_path = os.path.join(FRAME_DIR, frame_files[i])
                                    dst_img_path = os.path.join(UNIFIED_FRAME_DIR, f"{unique_name}.jpg")
                                    shutil.copy2(src_img_path, dst_img_path)
                                    
                                    # ä¿å­˜æ ‡æ³¨åˆ°ç»Ÿä¸€ç›®å½•
                                    label_file = os.path.join(UNIFIED_LABEL_DIR, f"{unique_name}.txt")
                                    with open(label_file, "w") as f:
                                        f.write(yolo_line)
                                    
                                    exported_count += 1
                        
                        # æ›´æ–°ç±»åˆ«æ–‡ä»¶
                        classes_file = os.path.join(UNIFIED_LABEL_DIR, "classes.txt")
                        with open(classes_file, "w") as f:
                            for label_name in st.session_state["label_history"]:
                                f.write(f"{label_name}\n")
                        
                        st.success(f"âœ… å·²æ·»åŠ  {exported_count} å¸§æ•°æ®åˆ°ç»Ÿä¸€æ•°æ®é›† '{dataset_name}'")

    elif work_mode == "ä¿®æ­£ç‰¹å®šå¸§":
        if "refine_frame_idx" not in st.session_state:
            st.warning("âš ï¸ è¯·å…ˆåœ¨'é¢„è§ˆ100å¸§æ©ç 'æ¨¡å¼ä¸‹é€‰æ‹©è¦ä¿®æ­£çš„å¸§")
        else:
            refine_frame_idx = st.session_state["refine_frame_idx"]
            st.subheader(f"ğŸ”§ ä¿®æ­£å¸§ {refine_frame_idx}")
            
            col1, col2 = st.columns([3, 1])
            
            with col1:
                frame_path = os.path.join(FRAME_DIR, frame_files[refine_frame_idx])
                img = np.array(Image.open(frame_path).convert("RGB"))
                
                # æ˜¾ç¤ºå½“å‰æ©ç 
                video_segments = st.session_state.get("video_segments", {})
                mask = video_segments.get(refine_frame_idx, {}).get(1, None)  # ä½¿ç”¨obj_id=1
                
                if mask is not None and mask.sum() > 0:
                    overlay = img.copy()
                    
                    # ç¡®ä¿æ©ç å½¢çŠ¶ä¸å›¾åƒåŒ¹é…
                    if len(mask.shape) == 3:
                        mask = mask.squeeze()
                    if len(mask.shape) == 1:
                        # å¦‚æœæ©ç æ˜¯1Dï¼Œéœ€è¦é‡æ–°reshape
                        h, w = img.shape[:2]
                        if mask.size == h * w:
                            mask = mask.reshape(h, w)
                        else:
                            st.error(f"æ©ç å¤§å°ä¸åŒ¹é…: æ©ç å¤§å°={mask.size}, å›¾åƒå¤§å°={h}x{w}")
                            st.image(img, caption=f"å¸§ {refine_frame_idx} - æ©ç å½¢çŠ¶é”™è¯¯")
                    
                    # ç¡®ä¿æ©ç æ˜¯äºŒå€¼çš„
                    mask = (mask > 0).astype(np.uint8)
                    
                    overlay[mask == 1] = (overlay[mask == 1] * 0.5 + np.array([128,128,255]) * 0.5).astype(np.uint8)
                    
                    # æ˜¾ç¤ºä¿®æ­£ç‚¹
                    refine_points = st.session_state.get("refine_points", [])
                    for x, y, l in refine_points:
                        color = (0, 255, 0) if l == 1 else (0, 0, 255)
                        cv2.circle(overlay, (int(x), int(y)), 8, color, -1)
                    
                    click = streamlit_image_coordinates(overlay, key=f"refine_{refine_frame_idx}")
                    if click:
                        st.session_state["last_refine_click"] = (click["x"], click["y"])
                    
                    st.image(overlay, caption=f"ä¿®æ­£å¸§ {refine_frame_idx} - ç‚¹å‡»æ·»åŠ ä¿®æ­£ç‚¹")
                else:
                    st.image(img, caption=f"å¸§ {refine_frame_idx} - æ— æ©ç ")
            
            with col2:
                st.markdown("### ğŸ”§ ä¿®æ­£æ“ä½œ")
                
                if "last_refine_click" in st.session_state:
                    col_a, col_b = st.columns(2)
                    with col_a:
                        if st.button("æ·»åŠ æ­£ç‚¹ä¿®æ­£"):
                            if "refine_points" not in st.session_state:
                                st.session_state["refine_points"] = []
                            st.session_state["refine_points"].append((*st.session_state["last_refine_click"], 1))
                            del st.session_state["last_refine_click"]
                            st.rerun()
                    
                    with col_b:
                        if st.button("æ·»åŠ è´Ÿç‚¹ä¿®æ­£"):
                            if "refine_points" not in st.session_state:
                                st.session_state["refine_points"] = []
                            st.session_state["refine_points"].append((*st.session_state["last_refine_click"], 0))
                            del st.session_state["last_refine_click"]
                            st.rerun()
                
                if st.button("åº”ç”¨ä¿®æ­£å¹¶é‡æ–°ä¼ æ’­"):
                    refine_points = st.session_state.get("refine_points", [])
                    if refine_points:
                        with st.spinner("æ­£åœ¨åº”ç”¨ä¿®æ­£å¹¶é‡æ–°ä¼ æ’­..."):
                            with torch.autocast(get_autocast_device()):
                                inference_state = st.session_state["inference_state"]
                                pts = [[p[0], p[1]] for p in refine_points]
                                lbls = [p[2] for p in refine_points]
                                
                                # åœ¨æŒ‡å®šå¸§æ·»åŠ ä¿®æ­£ç‚¹
                                sam2_model.add_new_points_or_box(
                                    inference_state=inference_state,
                                    frame_idx=refine_frame_idx,
                                    obj_id=1,  # ä½¿ç”¨obj_id=1
                                    points=pts,
                                    labels=lbls,
                                )
                                
                                # é‡æ–°ä¼ æ’­
                                video_segments = {}
                                for i, obj_ids, mask_logits in sam2_model.propagate_in_video(inference_state):
                                    video_segments[i] = {
                                        obj_id: (mask_logits[j] > 0).cpu().numpy()
                                        for j, obj_id in enumerate(obj_ids)
                                    }
                                    # é™åˆ¶åªå¤„ç†å‰100å¸§
                                    if i >= 99:
                                        break
                                
                                st.session_state["video_segments"] = video_segments
                                st.session_state["refine_points"] = []
                        st.success("âœ… ä¿®æ­£å®Œæˆå¹¶é‡æ–°ä¼ æ’­ï¼")
                    else:
                        st.warning("âš ï¸ è¯·å…ˆæ·»åŠ ä¿®æ­£ç‚¹")
                
                if st.button("æ¸…é™¤ä¿®æ­£ç‚¹"):
                    st.session_state["refine_points"] = []
                    st.success("âœ… ä¿®æ­£ç‚¹å·²æ¸…é™¤")
                    st.rerun()

# ä¾§è¾¹æ æ˜¾ç¤ºå½“å‰çŠ¶æ€
with st.sidebar:
    st.header("ğŸ“Š å½“å‰çŠ¶æ€")
    if current_session_id:
        st.write(f"ä¼šè¯ID: {current_session_id}")
        device = get_device()
        if device.type == "mps":
            st.write("è®¾å¤‡: ğŸš€ Apple Silicon MPS")
        elif device.type == "cuda":
            st.write("è®¾å¤‡: ğŸš€ CUDA GPU")
        else:
            st.write("è®¾å¤‡: ğŸ’» CPU")
        
        # GPUå†…å­˜ç›‘æ§
        if device.type == "cuda":
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            gpu_allocated = torch.cuda.memory_allocated(0) / 1024**3
            gpu_cached = torch.cuda.memory_reserved(0) / 1024**3
            st.write(f"GPUæ€»å†…å­˜: {gpu_memory:.1f} GB")
            st.write(f"GPUå·²åˆ†é…: {gpu_allocated:.1f} GB")
            st.write(f"GPUç¼“å­˜: {gpu_cached:.1f} GB")
            
            if st.button("ğŸ§¹ æ¸…ç†GPUå†…å­˜"):
                torch.cuda.empty_cache()
                st.success("âœ… GPUå†…å­˜å·²æ¸…ç†")
                st.rerun()
        
        # MPSå†…å­˜ç›‘æ§ï¼ˆApple Siliconï¼‰
        elif device.type == "mps":
            if st.button("ğŸ§¹ æ¸…ç†MPSå†…å­˜"):
                torch.mps.empty_cache()
                st.success("âœ… MPSå†…å­˜å·²æ¸…ç†")
                st.rerun()
        
        if "video_segments" in st.session_state:
            total_masks = sum(1 for frame_data in st.session_state["video_segments"].values() 
                            for mask in frame_data.values() if mask.sum() > 0)
            st.write(f"æœ‰æ•ˆæ©ç æ•°: {total_masks}")
        if "label_history" in st.session_state:
            st.write("æ ‡ç­¾å†å²:")
            for i, label in enumerate(st.session_state["label_history"]):
                st.write(f"{i}: {label}")
    
    # è®¾ç½®GPUå†…å­˜ç®¡ç†
    st.header("âš™ï¸ å†…å­˜è®¾ç½®")
    if st.button("è®¾ç½®GPUå†…å­˜ä¼˜åŒ–"):
        os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
        st.success("âœ… å·²è®¾ç½®GPUå†…å­˜ä¼˜åŒ–")
        st.write("é‡å¯åº”ç”¨ç”Ÿæ•ˆ") 