import os
# ç¦ç”¨torchç¼–è¯‘å™¨å’Œinductoræ¥é¿å…CUDAå›¾å½¢é”™è¯¯
os.environ["TORCH_COMPILE_DISABLE"] = "1"
os.environ["TORCHINDUCTOR_DISABLE"] = "1"

import uuid
import shutil
import numpy as np
import cv2
import streamlit as st

st.set_page_config(layout="wide")
from PIL import Image
import torch
from moviepy import VideoFileClip
from torchvision.ops import masks_to_boxes
from sam2.build_sam import build_sam2_video_predictor
from streamlit_image_coordinates import streamlit_image_coordinates

# æ™ºèƒ½è®¾å¤‡æ£€æµ‹å‡½æ•°
def get_device():
    """æ™ºèƒ½æ£€æµ‹æœ€ä½³å¯ç”¨è®¾å¤‡ï¼šMPS > CUDA > CPU"""
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        return torch.device("mps")
    elif torch.cuda.is_available():
        return torch.device("cuda")
    else:
        return torch.device("cpu")

def get_autocast_device():
    """è·å–ç”¨äºautocastçš„è®¾å¤‡ç±»å‹å­—ç¬¦ä¸²"""
    device = get_device()
    if device.type == "mps":
        return "cpu"  # MPSåœ¨autocastä¸­ä½¿ç”¨cpuæ¨¡å¼
    else:
        return device.type

# è¾¹ç•Œæ¡†ç”Ÿæˆå‡½æ•°
def get_tight_bbox(mask):
    """
    åŸºäºåƒç´ åˆ†å¸ƒçš„ç´§å¯†è¾¹ç•Œæ¡†
    """
    if mask.sum() == 0:
        return None
    
    # ç¡®ä¿æ©ç æ˜¯2D
    if len(mask.shape) == 3:
        mask = mask.squeeze()
    if len(mask.shape) != 2:
        return None
    
    # æ‰¾åˆ°æ‰€æœ‰å‰æ™¯åƒç´ çš„ä½ç½®
    y_indices, x_indices = np.where(mask > 0)
    
    if len(y_indices) == 0:
        return None
    
    # è®¡ç®—è¾¹ç•Œæ¡†
    x_min, x_max = x_indices.min(), x_indices.max()
    y_min, y_max = y_indices.min(), y_indices.max()
    
    return [x_min, y_min, x_max + 1, y_max + 1]

def get_contour_bbox(mask):
    """
    åŸºäºè½®å»“çš„æ›´ç²¾ç¡®è¾¹ç•Œæ¡†
    """
    if mask.sum() == 0:
        return None
    
    # ç¡®ä¿æ©ç æ˜¯2Dä¸”ä¸ºuint8ç±»å‹
    if len(mask.shape) == 3:
        mask = mask.squeeze()
    if len(mask.shape) != 2:
        return None
    
    # ç¡®ä¿æ©ç æ˜¯äºŒå€¼çš„uint8ç±»å‹
    mask_uint8 = (mask > 0).astype(np.uint8)
    
    # æ‰¾åˆ°è½®å»“
    contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    if not contours:
        return None
    
    # æ‰¾åˆ°æœ€å¤§çš„è½®å»“
    largest_contour = max(contours, key=cv2.contourArea)
    
    # è·å–è½®å»“çš„è¾¹ç•Œæ¡†
    x, y, w, h = cv2.boundingRect(largest_contour)
    
    return [x, y, x + w, y + h]

def get_bbox_by_method(mask, method="è½®å»“è¾¹ç•Œæ¡†"):
    """
    æ ¹æ®é€‰æ‹©çš„æ–¹æ³•ç”Ÿæˆè¾¹ç•Œæ¡†
    """
    if mask.sum() == 0:
        return None
    
    # ç¡®ä¿æ©ç æ˜¯2D
    if len(mask.shape) == 3:
        mask = mask.squeeze()
    if len(mask.shape) != 2:
        return None
    
    if method == "è½®å»“è¾¹ç•Œæ¡†":
        box = get_contour_bbox(mask)
        if box is None:
            box = get_tight_bbox(mask)
        return box
    elif method == "ç´§å¯†è¾¹ç•Œæ¡†":
        return get_tight_bbox(mask)
    elif method == "ä¼ ç»Ÿæ–¹æ³•":
        try:
            # ç¡®ä¿æ©ç æ˜¯2Dä¸”ä¸ºå¸ƒå°”ç±»å‹
            mask_2d = (mask > 0).astype(bool)
            box = masks_to_boxes(torch.tensor(mask_2d[None]))[0].int().tolist()
            return box
        except:
            return get_tight_bbox(mask)
    else:
        return get_contour_bbox(mask)

# åˆå§‹åŒ–æ¨¡å‹
@st.cache_resource
def load_sam2_model():
    checkpoint = os.path.join(os.path.expanduser("~"), "mysam", "sam2", "checkpoints", "sam2.1_hiera_base_plus.pt")
    model_cfg = "configs/sam2.1/sam2.1_hiera_b+.yaml"
    
    # æ™ºèƒ½è®¾å¤‡æ£€æµ‹ï¼šä¼˜å…ˆä½¿ç”¨MPSåŠ é€Ÿ (Apple Silicon)ï¼Œç„¶åCUDAï¼Œæœ€åCPU
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        device = torch.device("mps")
        print("ğŸš€ ä½¿ç”¨Apple Silicon MPSåŠ é€Ÿ")
        st.success("ğŸš€ æ­£åœ¨ä½¿ç”¨Apple Silicon MPSåŠ é€Ÿ")
    elif torch.cuda.is_available():
        device = torch.device("cuda")
        print("ğŸš€ ä½¿ç”¨CUDA GPUåŠ é€Ÿ")
        st.success("ğŸš€ æ­£åœ¨ä½¿ç”¨CUDA GPUåŠ é€Ÿ")
    else:
        device = torch.device("cpu")
        print("ğŸ’» ä½¿ç”¨CPUå¤„ç†")
        st.info("ğŸ’» æ­£åœ¨ä½¿ç”¨CPUå¤„ç†")
    
    return build_sam2_video_predictor(model_cfg, checkpoint, device=device, vos_optimized=True)

sam2_model = load_sam2_model()

st.title("ğŸ¬ SAM2 å¤šç›®æ ‡ä¸“ç”¨è§†é¢‘æ ‡æ³¨å·¥å…· & YOLO11æ ¼å¼å¯¼å‡º")

# ===== å¤šç›®æ ‡æ•°æ®ç»“æ„ç®¡ç† =====
def init_multi_target_data():
    """åˆå§‹åŒ–å¤šç›®æ ‡æ•°æ®ç»“æ„"""
    if "multi_target_data" not in st.session_state:
        st.session_state["multi_target_data"] = {}
    if "next_obj_id" not in st.session_state:
        st.session_state["next_obj_id"] = 1
    if "current_active_label" not in st.session_state:
        st.session_state["current_active_label"] = None
    if "current_active_object" not in st.session_state:
        st.session_state["current_active_object"] = None

def add_new_label(label_name):
    """æ·»åŠ æ–°æ ‡ç­¾"""
    if label_name and label_name not in st.session_state["multi_target_data"]:
        st.session_state["multi_target_data"][label_name] = {"objects": {}}
        return True
    return False

def add_new_object_to_label(label_name):
    """ä¸ºæŒ‡å®šæ ‡ç­¾æ·»åŠ æ–°ç›®æ ‡"""
    if label_name in st.session_state["multi_target_data"]:
        obj_id = st.session_state["next_obj_id"]
        st.session_state["multi_target_data"][label_name]["objects"][obj_id] = {
            "ann_obj_id": obj_id,
            "points": {},  # {frame_idx: [(x, y, label), ...]}
            "masks": {},   # å­˜å‚¨ç”Ÿæˆçš„æ©ç 
        }
        st.session_state["next_obj_id"] += 1
        return obj_id
    return None

def get_current_active_object():
    """è·å–å½“å‰æ¿€æ´»çš„ç›®æ ‡æ•°æ®"""
    active_label = st.session_state["current_active_label"]
    active_obj = st.session_state["current_active_object"]
    
    if (active_label and active_obj and 
        active_label in st.session_state["multi_target_data"] and
        active_obj in st.session_state["multi_target_data"][active_label]["objects"]):
        return st.session_state["multi_target_data"][active_label]["objects"][active_obj]
    return None

def add_point_to_current_object(frame_idx, x, y, label):
    """ä¸ºå½“å‰æ¿€æ´»çš„ç›®æ ‡æ·»åŠ ç‚¹"""
    obj_data = get_current_active_object()
    if obj_data is not None:
        if frame_idx not in obj_data["points"]:
            obj_data["points"][frame_idx] = []
        obj_data["points"][frame_idx].append((x, y, label))
        return True
    return False

# åˆå§‹åŒ–å¤šç›®æ ‡æ•°æ®
init_multi_target_data()

def preview_all_targets_only(frame_index, frame_np, frame_files, current_session_id):
    """åªé¢„è§ˆå½“å‰å¸§æ‰€æœ‰ç›®æ ‡çš„åˆ†å‰²æ•ˆæœï¼Œä¸ä¿å­˜æ•°æ®"""
    multi_target_data = st.session_state.get("multi_target_data", {})
    FRAME_DIR = f"frame_cache_{current_session_id}"
    
    colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255), (0, 255, 255)]
    overlay = frame_np.copy()
    
    try:
        device = get_device()
        if device.type == "cuda":
            torch.cuda.empty_cache()
        elif device.type == "mps":
            torch.mps.empty_cache()
        
        with torch.autocast(get_autocast_device()):
            inference_state = sam2_model.init_state(video_path=FRAME_DIR)
            
            color_idx = 0
            processed_targets = 0
            for label_name, label_data in multi_target_data.items():
                for obj_id, obj_data in label_data["objects"].items():
                    if frame_index in obj_data["points"] and obj_data["points"][frame_index]:
                        obj_points = obj_data["points"][frame_index]
                        pts = [[p[0], p[1]] for p in obj_points]
                        lbls = [p[2] for p in obj_points]
                        
                        # ä¸ºæ¯ä¸ªç›®æ ‡ç”Ÿæˆæ©ç ï¼Œä½¿ç”¨æ­£ç¡®çš„obj_id
                        _, _, mask_logits = sam2_model.add_new_points_or_box(
                            inference_state=inference_state,
                            frame_idx=frame_index,
                            obj_id=obj_id,
                            points=np.array(pts, dtype=np.float32),
                            labels=np.array(lbls, dtype=np.int32),
                        )
                        
                        binary_mask = (mask_logits[0] > 0).cpu().numpy().squeeze()
                        
                        if binary_mask.sum() > 0:
                            # ä¸ºæ¯ä¸ªç›®æ ‡ä½¿ç”¨ä¸åŒé¢œè‰²
                            target_color = colors[color_idx % len(colors)]
                            overlay[binary_mask == 1] = (overlay[binary_mask == 1] * 0.6 + np.array(target_color) * 0.4).astype(np.uint8)
                            
                            # ç»˜åˆ¶è¾¹ç•Œæ¡†
                            bbox_method = st.session_state.get("bbox_method", "ä¼ ç»Ÿæ–¹æ³•")
                            box = get_bbox_by_method(binary_mask, bbox_method)
                            
                            if box is not None:
                                x1, y1, x2, y2 = box
                                cv2.rectangle(overlay, (x1, y1), (x2, y2), target_color, 2)
                                # æ ‡æ³¨ç›®æ ‡ä¿¡æ¯
                                cv2.putText(overlay, f"{label_name}-{obj_id}", (x1, y1-10), 
                                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, target_color, 2)
                            
                            processed_targets += 1
                        
                        # ç»˜åˆ¶æ ‡æ³¨ç‚¹
                        for i, (x, y, l) in enumerate(obj_points):
                            color = colors[color_idx % len(colors)]
                            radius = 10 if l == 1 else 5
                            cv2.circle(overlay, (int(x), int(y)), radius, color, -1)
                            cv2.putText(overlay, str(obj_id), (int(x)+10, int(y)-10), 
                                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
                        
                        color_idx += 1
                        
                        # æ¸…ç†å†…å­˜
                        del mask_logits
                        if device.type == "cuda":
                            torch.cuda.empty_cache()
                        elif device.type == "mps":
                            torch.mps.empty_cache()
            
            st.image(overlay, caption=f"å…¨éƒ¨ç›®æ ‡é¢„è§ˆ - å·²å¤„ç†{processed_targets}ä¸ªç›®æ ‡")
            st.info(f"ğŸ‘ï¸ é¢„è§ˆå®Œæˆï¼æ˜¾ç¤ºäº†{processed_targets}ä¸ªç›®æ ‡çš„åˆ†å‰²æ•ˆæœ")
    
    except Exception as e:
        st.error(f"âŒ é¢„è§ˆå¤±è´¥: {str(e)}")
        import traceback
        st.code(traceback.format_exc())

def save_all_targets_in_frame(frame_index, frame_np, frame_files, current_session_id):
    """ä¿å­˜å½“å‰å¸§æ‰€æœ‰ç›®æ ‡çš„æ•°æ®"""
    multi_target_data = st.session_state.get("multi_target_data", {})
    FRAME_DIR = f"frame_cache_{current_session_id}"
    UNIFIED_FRAME_DIR = f"frames_{st.session_state.get('dataset_name', 'lajiao_dataset')}"
    UNIFIED_LABEL_DIR = f"labels_{st.session_state.get('dataset_name', 'lajiao_dataset')}"
    
    all_labels = list(multi_target_data.keys())
    saved_count = 0
    
    try:
        device = get_device()
        if device.type == "cuda":
            torch.cuda.empty_cache()
        elif device.type == "mps":
            torch.mps.empty_cache()
        
        with torch.autocast(get_autocast_device()):
            inference_state = sam2_model.init_state(video_path=FRAME_DIR)
            
            for label_name, label_data in multi_target_data.items():
                for obj_id, obj_data in label_data["objects"].items():
                    if frame_index in obj_data["points"] and obj_data["points"][frame_index]:
                        obj_points = obj_data["points"][frame_index]
                        pts = [[p[0], p[1]] for p in obj_points]
                        lbls = [p[2] for p in obj_points]
                        
                        # ä¸ºæ¯ä¸ªç›®æ ‡ç”Ÿæˆæ©ç 
                        _, _, mask_logits = sam2_model.add_new_points_or_box(
                            inference_state=inference_state,
                            frame_idx=frame_index,
                            obj_id=obj_id,
                            points=np.array(pts, dtype=np.float32),
                            labels=np.array(lbls, dtype=np.int32),
                        )
                        
                        binary_mask = (mask_logits[0] > 0).cpu().numpy().squeeze()
                        
                        if binary_mask.sum() > 0:
                            # è®¡ç®—è¾¹ç•Œæ¡†
                            bbox_method = st.session_state.get("bbox_method", "ä¼ ç»Ÿæ–¹æ³•")
                            box = get_bbox_by_method(binary_mask, bbox_method)
                            
                            if box is not None:
                                x1, y1, x2, y2 = box
                                
                                # ä¿å­˜æ•°æ®
                                label_id = all_labels.index(label_name)
                                h, w = binary_mask.shape
                                yolo_line = f"{label_id} {(x1+x2)/2/w:.6f} {(y1+y2)/2/h:.6f} {(x2-x1)/w:.6f} {(y2-y1)/h:.6f}\n"
                                
                                base_name = frame_files[frame_index].replace(".jpg", "")
                                unique_name = f"{current_session_id}_{base_name}_obj{obj_id}"
                                
                                # ä¿å­˜å›¾åƒå’Œæ ‡æ³¨
                                src_img_path = os.path.join(FRAME_DIR, frame_files[frame_index])
                                dst_img_path = os.path.join(UNIFIED_FRAME_DIR, f"{unique_name}.jpg")
                                shutil.copy2(src_img_path, dst_img_path)
                                
                                label_file = os.path.join(UNIFIED_LABEL_DIR, f"{unique_name}.txt")
                                with open(label_file, "w") as f:
                                    f.write(yolo_line)
                                
                                saved_count += 1
                        
                        # æ¸…ç†å†…å­˜
                        del mask_logits
                        if device.type == "cuda":
                            torch.cuda.empty_cache()
                        elif device.type == "mps":
                            torch.mps.empty_cache()
            
            # æ›´æ–°ç±»åˆ«æ–‡ä»¶
            if saved_count > 0:
                classes_file = os.path.join(UNIFIED_LABEL_DIR, "classes.txt")
                with open(classes_file, "w") as f:
                    for label_name in all_labels:
                        f.write(f"{label_name}\n")
            
            st.success(f"ğŸ’¾ ä¿å­˜å®Œæˆï¼å·²ä¿å­˜{saved_count}ä¸ªç›®æ ‡æ•°æ®")
    
    except Exception as e:
        st.error(f"âŒ ä¿å­˜å¤±è´¥: {str(e)}")
        import traceback
        st.code(traceback.format_exc())

def generate_100_frames_all_targets():
    """ä¸€é”®ç”Ÿæˆ100å¸§å…¨éƒ¨ç›®æ ‡çš„æ©ç """
    multi_target_data = st.session_state.get("multi_target_data", {})
    current_session_id = st.session_state.get("current_session_id")
    FRAME_DIR = f"frame_cache_{current_session_id}"
    
    # æ‰¾åˆ°æœ‰æ ‡æ³¨ç‚¹çš„ç›®æ ‡
    targets_with_points = {}
    for label_name, label_data in multi_target_data.items():
        for obj_id, obj_data in label_data["objects"].items():
            # ä½¿ç”¨å®é™…çš„ann_obj_idä½œä¸ºé”®
            ann_obj_id = obj_data["ann_obj_id"]
            for frame_idx, points in obj_data["points"].items():
                if points:  # æœ‰æ ‡æ³¨ç‚¹
                    if ann_obj_id not in targets_with_points:
                        targets_with_points[ann_obj_id] = {
                            "label": label_name,
                            "frame": frame_idx,
                            "points": points,
                            "original_obj_id": obj_id  # ä¿å­˜åŸå§‹obj_idç”¨äºè°ƒè¯•
                        }
                    break  # åªéœ€è¦ç¬¬ä¸€ä¸ªæœ‰ç‚¹çš„å¸§ä½œä¸ºå‚è€ƒ
    
    if not targets_with_points:
        st.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æœ‰æ ‡æ³¨ç‚¹çš„ç›®æ ‡")
        return None
    
    st.write(f"ğŸ” æ‰¾åˆ°{len(targets_with_points)}ä¸ªç›®æ ‡æœ‰æ ‡æ³¨ç‚¹")
    
    try:
        device = get_device()
        if device.type == "cuda":
            torch.cuda.empty_cache()
        elif device.type == "mps":
            torch.mps.empty_cache()
        
        with torch.autocast(get_autocast_device()):
            inference_state = sam2_model.init_state(video_path=FRAME_DIR)
            
            # ä¸ºæ¯ä¸ªç›®æ ‡æ·»åŠ å‚è€ƒç‚¹
            for ann_obj_id, target_info in targets_with_points.items():
                ref_frame = target_info["frame"]
                ref_points = target_info["points"]
                pts = [[p[0], p[1]] for p in ref_points]
                lbls = [p[2] for p in ref_points]
                
                st.write(f"ğŸ¯ å¤„ç†ç›®æ ‡ann_obj_id={ann_obj_id}({target_info['label']}) - å‚è€ƒå¸§{ref_frame}")
                
                sam2_model.add_new_points_or_box(
                    inference_state=inference_state,
                    frame_idx=ref_frame,
                    obj_id=ann_obj_id,  # ä½¿ç”¨ann_obj_id
                    points=np.array(pts, dtype=np.float32),
                    labels=np.array(lbls, dtype=np.int32),
                )
            
            # æ‰¹é‡ä¼ æ’­æ‰€æœ‰ç›®æ ‡
            video_segments = {}
            for i, obj_ids, mask_logits in sam2_model.propagate_in_video(inference_state):
                # ç¡®ä¿obj_idså’Œmask_logitsçš„å¯¹åº”å…³ç³»æ­£ç¡®
                frame_masks = {}
                st.write(f"ğŸ” å¸§{i}: obj_ids={obj_ids}, maskæ•°é‡={len(mask_logits)}")
                
                for j, obj_id in enumerate(obj_ids):
                    if j < len(mask_logits):
                        mask = (mask_logits[j] > 0).cpu().numpy()
                        frame_masks[obj_id] = mask
                        st.write(f"  â€¢ ç›®æ ‡{obj_id}: æ©ç å¤§å°={mask.sum()}")
                    else:
                        st.warning(f"  âš ï¸ ç›®æ ‡{obj_id}: ç´¢å¼•{j}è¶…å‡ºmask_logitsèŒƒå›´")
                
                video_segments[i] = frame_masks
                
                # ç«‹å³æ¸…ç†å†…å­˜
                if device.type == "cuda":
                    torch.cuda.empty_cache()
                elif device.type == "mps":
                    torch.mps.empty_cache()
                
                # é™åˆ¶å¤„ç†100å¸§
                if i >= 99:
                    break
            
            # ç»Ÿè®¡ç»“æœ
            total_masks = sum(1 for frame_data in video_segments.values() 
                            for mask in frame_data.values() if mask.sum() > 0)
            
            st.write(f"âœ… æˆåŠŸç”Ÿæˆ{total_masks}ä¸ªæœ‰æ•ˆæ©ç ")
            
            # ä¿å­˜ç»“æœåˆ°session state
            st.session_state["video_segments"] = video_segments
            st.session_state["inference_state"] = inference_state
            st.session_state["multi_target_reference"] = targets_with_points
            
            # æ‰“å°ä¿å­˜çš„å‚è€ƒä¿¡æ¯ç”¨äºè°ƒè¯•
            st.write("ğŸ” ä¿å­˜çš„å¤šç›®æ ‡å‚è€ƒä¿¡æ¯:")
            for ann_obj_id, info in targets_with_points.items():
                st.write(f"  â€¢ ann_obj_id={ann_obj_id}: æ ‡ç­¾={info['label']}, å‚è€ƒå¸§={info['frame']}, åŸå§‹obj_id={info.get('original_obj_id', 'æœªçŸ¥')}")
            
            return video_segments
    
    except Exception as e:
        st.error(f"âŒ ç”Ÿæˆ100å¸§æ©ç å¤±è´¥: {str(e)}")
        import traceback
        st.code(traceback.format_exc())
        return None

# ä¾§è¾¹æ ï¼šæ•°æ®é›†ç®¡ç†
with st.sidebar:
    st.header("ğŸ“ æ•°æ®é›†ç®¡ç†")
    
    # æ•°æ®é›†åç§°è®¾ç½®
    dataset_name = st.text_input(
        "æ•°æ®é›†åç§°", 
        value=st.session_state.get("dataset_name", "lajiao_dataset"),
        help="æ‰€æœ‰è§†é¢‘çš„æ ‡æ³¨æ•°æ®å°†ç»Ÿä¸€ä¿å­˜åˆ°è¿™ä¸ªæ•°æ®é›†ä¸­"
    )
    st.session_state["dataset_name"] = dataset_name
    
    # æ˜¾ç¤ºå½“å‰æ•°æ®é›†ç»Ÿè®¡
    unified_frame_dir = f"frames_{dataset_name}"
    unified_label_dir = f"labels_{dataset_name}"
    
    if os.path.exists(unified_frame_dir):
        frame_count = len([f for f in os.listdir(unified_frame_dir) if f.endswith('.jpg')])
        st.metric("ğŸ“· æ€»å›¾åƒæ•°", frame_count)
    else:
        st.metric("ğŸ“· æ€»å›¾åƒæ•°", 0)
    
    if os.path.exists(unified_label_dir):
        label_count = len([f for f in os.listdir(unified_label_dir) if f.endswith('.txt')])
        st.metric("ğŸ·ï¸ æ€»æ ‡æ³¨æ•°", label_count)
    else:
        st.metric("ğŸ·ï¸ æ€»æ ‡æ³¨æ•°", 0)
    
    # æ•°æ®é›†æ“ä½œæŒ‰é’®
    if st.button("ğŸ”„ åˆ·æ–°ç»Ÿè®¡"):
        st.rerun()
    
    if st.button("ğŸ“¦ å¯¼å‡ºå½“å‰æ•°æ®é›†"):
        if os.path.exists(unified_frame_dir) and os.path.exists(unified_label_dir):
            import zipfile
            zip_path = f"{dataset_name}_export.zip"
            with zipfile.ZipFile(zip_path, 'w') as zipf:
                # æ·»åŠ å›¾åƒæ–‡ä»¶
                for file in os.listdir(unified_frame_dir):
                    if file.endswith('.jpg'):
                        zipf.write(os.path.join(unified_frame_dir, file), f"images/{file}")
                # æ·»åŠ æ ‡æ³¨æ–‡ä»¶
                for file in os.listdir(unified_label_dir):
                    if file.endswith('.txt'):
                        zipf.write(os.path.join(unified_label_dir, file), f"labels/{file}")
            st.success(f"âœ… æ•°æ®é›†å·²å¯¼å‡ºä¸º: {zip_path}")
        else:
            st.warning("âš ï¸ æ•°æ®é›†ä¸ºç©ºï¼Œæ— æ³•å¯¼å‡º")
    
    st.divider()
    
    # è¾¹ç•Œæ¡†ç®—æ³•é€‰æ‹©
    st.header("âš™ï¸ ç®—æ³•é…ç½®")
    bbox_method = st.selectbox(
        "è¾¹ç•Œæ¡†ç®—æ³•",
        ["ä¼ ç»Ÿæ–¹æ³•", "è½®å»“è¾¹ç•Œæ¡†", "ç´§å¯†è¾¹ç•Œæ¡†"],
        help="ä¼ ç»Ÿæ–¹æ³•ï¼šæœ€å°å¤–æ¥çŸ©å½¢ï¼Œæ•ˆæœæœ€å¥½\\nè½®å»“è¾¹ç•Œæ¡†ï¼šåŸºäºç‰©ä½“è½®å»“\\nç´§å¯†è¾¹ç•Œæ¡†ï¼šåŸºäºåƒç´ åˆ†å¸ƒ"
    )
    st.session_state["bbox_method"] = bbox_method

    st.divider()
    
    # ===== å¤šç›®æ ‡ç®¡ç†ç•Œé¢ =====
    st.header("ğŸ·ï¸ å¤šç›®æ ‡ç®¡ç†")
    
    # æ˜¾ç¤ºå½“å‰å¤šç›®æ ‡æ•°æ®ç»“æ„
    multi_target_data = st.session_state["multi_target_data"]
    active_label = st.session_state["current_active_label"]
    active_obj = st.session_state["current_active_object"]
    
    if not multi_target_data:
        st.info("ğŸ’¡ æ·»åŠ ç¬¬ä¸€ä¸ªæ ‡ç­¾å¼€å§‹å¤šç›®æ ‡æ ‡æ³¨")
    
    # æ˜¾ç¤ºæ‰€æœ‰æ ‡ç­¾å’Œç›®æ ‡
    for label_name in multi_target_data.keys():
        label_objects = multi_target_data[label_name]["objects"]
        
        # æ ‡ç­¾è¡Œ
        col1, col2, col3 = st.columns([3, 1, 1])
        with col1:
            is_active_label = (active_label == label_name)
            label_style = "ğŸŸ¢" if is_active_label else "âšª"
            st.write(f"{label_style} **{label_name}**")
        
        with col2:
            # æ·»åŠ æ–°ç›®æ ‡
            if st.button("â•", key=f"add_obj_{label_name}", help=f"ä¸º{label_name}æ·»åŠ æ–°ç›®æ ‡"):
                new_obj_id = add_new_object_to_label(label_name)
                if new_obj_id:
                    st.session_state["current_active_label"] = label_name
                    st.session_state["current_active_object"] = new_obj_id
                    st.success(f"âœ… å·²ä¸º'{label_name}'æ·»åŠ ç›®æ ‡{new_obj_id}")
                    st.rerun()
        
        with col3:
            # åˆ é™¤æ ‡ç­¾
            if st.button("ğŸ—‘ï¸", key=f"del_label_{label_name}", help=f"åˆ é™¤æ ‡ç­¾{label_name}"):
                if label_name in st.session_state["multi_target_data"]:
                    del st.session_state["multi_target_data"][label_name]
                    if st.session_state["current_active_label"] == label_name:
                        st.session_state["current_active_label"] = None
                        st.session_state["current_active_object"] = None
                    st.warning(f"âš ï¸ å·²åˆ é™¤æ ‡ç­¾'{label_name}'")
                    st.rerun()
        
        # æ˜¾ç¤ºæ­¤æ ‡ç­¾ä¸‹çš„æ‰€æœ‰ç›®æ ‡
        if label_objects:
            for obj_id, obj_data in label_objects.items():
                col1, col2, col3 = st.columns([3, 1, 1])
                
                with col1:
                    is_active_obj = (active_obj == obj_id and active_label == label_name)
                    obj_style = "ğŸ”´" if is_active_obj else "â­•"
                    point_count = sum(len(points) for points in obj_data["points"].values())
                    st.write(f"    {obj_style} ç›®æ ‡{obj_id} ({point_count}ç‚¹)")
                
                with col2:
                    # æ¿€æ´»æ­¤ç›®æ ‡
                    if st.button("ğŸ“", key=f"activate_{label_name}_{obj_id}", help=f"æ¿€æ´»ç›®æ ‡{obj_id}"):
                        st.session_state["current_active_label"] = label_name
                        st.session_state["current_active_object"] = obj_id
                        st.success(f"âœ… å·²æ¿€æ´»'{label_name}'çš„ç›®æ ‡{obj_id}")
                        st.rerun()
                
                with col3:
                    # åˆ é™¤æ­¤ç›®æ ‡
                    if st.button("âŒ", key=f"del_obj_{label_name}_{obj_id}", help=f"åˆ é™¤ç›®æ ‡{obj_id}"):
                        if obj_id in st.session_state["multi_target_data"][label_name]["objects"]:
                            del st.session_state["multi_target_data"][label_name]["objects"][obj_id]
                            if st.session_state["current_active_object"] == obj_id:
                                st.session_state["current_active_object"] = None
                            st.warning(f"âš ï¸ å·²åˆ é™¤ç›®æ ‡{obj_id}")
                            st.rerun()
        
        st.write("")  # é—´è·
    
    # æ·»åŠ æ–°æ ‡ç­¾
    st.subheader("â• æ·»åŠ æ–°æ ‡ç­¾")
    new_label_name = st.text_input("æ ‡ç­¾åç§°", placeholder="ä¾‹å¦‚: niurou, tudou", key="new_label_input")
    
    col1, col2 = st.columns(2)
    with col1:
        if st.button("âœ… æ·»åŠ æ ‡ç­¾", key="add_new_label"):
            if new_label_name.strip():
                if add_new_label(new_label_name.strip().lower()):
                    st.success(f"âœ… å·²æ·»åŠ æ ‡ç­¾'{new_label_name}'")
                    st.rerun()
                else:
                    st.error("âŒ æ ‡ç­¾å·²å­˜åœ¨")
            else:
                st.warning("âš ï¸ è¯·è¾“å…¥æ ‡ç­¾åç§°")
    
    with col2:
        if st.button("ğŸ”„ é‡ç½®æ‰€æœ‰ç›®æ ‡", key="reset_multi_targets"):
            st.session_state["multi_target_data"] = {}
            st.session_state["next_obj_id"] = 1
            st.session_state["current_active_label"] = None
            st.session_state["current_active_object"] = None
            st.success("âœ… å·²é‡ç½®æ‰€æœ‰ç›®æ ‡")
            st.rerun()
    
    # å½“å‰æ¿€æ´»çŠ¶æ€
    st.subheader("ğŸ¯ å½“å‰æ¿€æ´»çŠ¶æ€")
    if active_label and active_obj:
        st.success(f"ğŸ“ æ ‡ç­¾: **{active_label}**")
        st.success(f"ğŸ¯ ç›®æ ‡: **{active_obj}** (ID: {active_obj})")
        
        obj_data = get_current_active_object()
        if obj_data:
            total_points = sum(len(points) for points in obj_data["points"].values())
            st.info(f"ğŸ“Š å·²æ ‡æ³¨ç‚¹æ•°: {total_points}")
    else:
        st.warning("âš ï¸ è¯·æ¿€æ´»ä¸€ä¸ªç›®æ ‡å¼€å§‹æ ‡æ³¨")

VIDEO_DIR = "video_segments"
os.makedirs(VIDEO_DIR, exist_ok=True)

# ä¸Šä¼ è§†é¢‘
uploaded_video = st.file_uploader("ğŸ“ ä¸Šä¼ å®Œæ•´åŸå§‹è§†é¢‘", type=["mp4"])
if uploaded_video:
    original_path = "temp_uploaded.mp4"
    with open(original_path, "wb") as f:
        f.write(uploaded_video.read())

    cap = cv2.VideoCapture(original_path)
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    
    # é˜²æ­¢é™¤é›¶é”™è¯¯
    if fps == 0:
        fps = 30  # é»˜è®¤å¸§ç‡
        st.warning("âš ï¸ æ— æ³•è·å–è§†é¢‘å¸§ç‡ï¼Œä½¿ç”¨é»˜è®¤å€¼30fps")
    
    duration = frame_count // fps
    st.video(original_path)

    st.subheader("âœ‚ï¸ è§†é¢‘è£å‰ª")
    start_time = st.slider("èµ·å§‹æ—¶é—´ï¼ˆç§’ï¼‰", 0, duration - 1, 0)
    end_time = st.slider("ç»“æŸæ—¶é—´ï¼ˆç§’ï¼‰", start_time + 1, duration, start_time + 5)

    if st.button("è£å‰ªå¹¶ä¿å­˜ç‰‡æ®µ"):
        clip = VideoFileClip(original_path).subclipped(start_time, end_time)
        
        # ä½¿ç”¨ç»Ÿä¸€çš„æ•°æ®é›†åç§°ï¼Œè€Œä¸æ˜¯éšæœºsession_id
        dataset_name = st.session_state.get("dataset_name", "lajiao_dataset")
        session_id = uuid.uuid4().hex[:8]  # åªç”¨äºè§†é¢‘æ–‡ä»¶å‘½å
        segment_path = os.path.join(VIDEO_DIR, f"{session_id}.mp4")
        clip.write_videofile(segment_path, codec="libx264")
        st.success(f"âœ… è§†é¢‘è£å‰ªå®Œæˆ: {segment_path}")
        st.session_state["segment_path"] = segment_path
        st.session_state["current_session_id"] = session_id  # å½“å‰è§†é¢‘çš„ID
        cap.release()
        shutil.move(original_path, f"{original_path}.bak")

# è§†é¢‘æ‹†å¸§ + åŠ è½½çŠ¶æ€
current_session_id = st.session_state.get("current_session_id", None)
segment_path = st.session_state.get("segment_path", None)
dataset_name = st.session_state.get("dataset_name", "lajiao_dataset")

# ä½¿ç”¨ç»Ÿä¸€çš„ç›®å½•åç§°
UNIFIED_FRAME_DIR = f"frames_{dataset_name}"
UNIFIED_LABEL_DIR = f"labels_{dataset_name}"

if current_session_id and segment_path:
    # åˆ›å»ºç»Ÿä¸€çš„æ•°æ®ç›®å½•
    os.makedirs(UNIFIED_FRAME_DIR, exist_ok=True)
    os.makedirs(UNIFIED_LABEL_DIR, exist_ok=True)
    
    # ä¸ºå½“å‰è§†é¢‘åˆ›å»ºä¸´æ—¶å¸§ç›®å½•
    FRAME_DIR = f"frame_cache_{current_session_id}"
    os.makedirs(FRAME_DIR, exist_ok=True)

    if not os.listdir(FRAME_DIR):
        cap = cv2.VideoCapture(segment_path)
        frame_idx = 0
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            frame_path = os.path.join(FRAME_DIR, f"{frame_idx:04d}.jpg")
            cv2.imwrite(frame_path, frame)
            frame_idx += 1
        cap.release()
        st.success(f"âœ… å…±æå– {frame_idx} å¸§è‡³ç¼“å­˜ç›®å½• {FRAME_DIR}")

    frame_files = sorted(os.listdir(FRAME_DIR))
    
    # æ·»åŠ å·¥ä½œæ¨¡å¼é€‰æ‹©
    st.subheader("ğŸ”§ å·¥ä½œæ¨¡å¼é€‰æ‹©")
    work_mode = st.radio("é€‰æ‹©å·¥ä½œæ¨¡å¼", ["åˆå§‹æ ‡æ³¨", "é¢„è§ˆ100å¸§æ©ç ", "ä¿®æ­£ç‰¹å®šå¸§"], horizontal=True)
    
    if work_mode == "åˆå§‹æ ‡æ³¨":
        frame_index = st.session_state.get("frame_index", 0)
        current_frame_path = os.path.join(FRAME_DIR, frame_files[frame_index])
        current_img = Image.open(current_frame_path)

        frame_np = np.array(current_img.convert("RGB"))
        preview_img = frame_np.copy()

        # æ˜¾ç¤ºå¤šç›®æ ‡çš„ç‚¹
        multi_target_data = st.session_state.get("multi_target_data", {})
        colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255), (0, 255, 255)]
        color_idx = 0
        
        for label_name, label_data in multi_target_data.items():
            for obj_id, obj_data in label_data["objects"].items():
                if frame_index in obj_data["points"]:
                    obj_points = obj_data["points"][frame_index]
                    color = colors[color_idx % len(colors)]
                    
                    for x, y, l in obj_points:
                        # æ­£ç‚¹ç”¨å¤§åœ†åœˆï¼Œè´Ÿç‚¹ç”¨å°åœ†åœˆ
                        radius = 8 if l == 1 else 4
                        cv2.circle(preview_img, (int(x), int(y)), radius, color, -1)
                        # åœ¨ç‚¹æ—è¾¹æ˜¾ç¤ºç›®æ ‡ID
                        cv2.putText(preview_img, f"{label_name}-{obj_id}", (int(x)+10, int(y)-10), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
                
                color_idx += 1

        # å·¦å›¾å³æ§
        col1, col2 = st.columns([3, 1])

        with col1:
            # é¢„è§ˆå¸§å¹¶ç‚¹å‡»æ‰“ç‚¹ - å›¾åƒå°ºå¯¸ä¼˜åŒ–
            img_height, img_width = preview_img.shape[:2]
            max_width = 800
            scale = 1.0
            if img_width > max_width:
                scale = max_width / img_width
                new_width = max_width
                new_height = int(img_height * scale)
                preview_img_display = cv2.resize(preview_img, (new_width, new_height))
            else:
                preview_img_display = preview_img
            
            click = streamlit_image_coordinates(preview_img_display, key=f"frame_{frame_index}_{st.session_state.get('refresh_flag', False)}")
            if click:
                # åæ ‡æ˜ å°„å›åŸå§‹å°ºå¯¸
                click_x = click["x"] / scale
                click_y = click["y"] / scale
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ¿€æ´»çš„å¤šç›®æ ‡
                active_label = st.session_state.get("current_active_label")
                active_obj = st.session_state.get("current_active_object")
                
                if active_label and active_obj:
                    # å¤šç›®æ ‡æ¨¡å¼
                    st.session_state["last_multi_click"] = (click_x, click_y)
                    st.session_state["last_multi_label"] = active_label
                    st.session_state["last_multi_obj"] = active_obj
                else:
                    st.warning("âš ï¸ è¯·å…ˆåœ¨ä¾§è¾¹æ æ¿€æ´»è¦æ ‡æ³¨çš„æ ‡ç­¾å’Œç›®æ ‡")

        with col2:
            st.markdown("### ğŸï¸ å½“å‰å¸§æ§åˆ¶")
            frame_index = st.slider("å¸§ä½ç½®", 0, len(frame_files) - 1, value=frame_index, key="frame_index")
            st.write(f"å½“å‰å¸§ç¼–å·ï¼š**{frame_index}**")



            # å¤šç›®æ ‡æ¨¡å¼ç‚¹å‡»å¤„ç†
            if "last_multi_click" in st.session_state:
                st.subheader("ğŸ¯ å¤šç›®æ ‡ç‚¹å‡»å¤„ç†")
                multi_label = st.session_state["last_multi_label"]
                multi_obj = st.session_state["last_multi_obj"]
                st.write(f"æ ‡ç­¾: **{multi_label}** | ç›®æ ‡: **{multi_obj}**")
                
                col_a, col_b = st.columns(2)
                with col_a:
                    if st.button("ä¿ç•™ç‚¹(æ­£ç‚¹)", key="multi_positive"):
                        if add_point_to_current_object(frame_index, *st.session_state["last_multi_click"], 1):
                            st.success(f"âœ… å·²ä¸º'{multi_label}'çš„ç›®æ ‡{multi_obj}æ·»åŠ æ­£ç‚¹")
                        st.session_state["refresh_flag"] = not st.session_state.get("refresh_flag", False)
                        del st.session_state["last_multi_click"]
                        del st.session_state["last_multi_label"]
                        del st.session_state["last_multi_obj"]
                        st.rerun()
                with col_b:
                    if st.button("å»é™¤ç‚¹(è´Ÿç‚¹)", key="multi_negative"):
                        if add_point_to_current_object(frame_index, *st.session_state["last_multi_click"], 0):
                            st.success(f"âœ… å·²ä¸º'{multi_label}'çš„ç›®æ ‡{multi_obj}æ·»åŠ è´Ÿç‚¹")
                        st.session_state["refresh_flag"] = not st.session_state.get("refresh_flag", False)
                        del st.session_state["last_multi_click"]
                        del st.session_state["last_multi_label"]
                        del st.session_state["last_multi_obj"]
                        st.rerun()
                st.divider()


            
            # ===== å¤šç›®æ ‡é¢„è§ˆåŠŸèƒ½ =====
            st.subheader("ğŸ¯ å¤šç›®æ ‡é¢„è§ˆ")
            
            # ç»Ÿè®¡å½“å‰å¸§çš„ç›®æ ‡
            multi_target_data = st.session_state.get("multi_target_data", {})
            frame_targets = []
            for label_name, label_data in multi_target_data.items():
                for obj_id, obj_data in label_data["objects"].items():
                    if frame_index in obj_data["points"] and obj_data["points"][frame_index]:
                        frame_targets.append((label_name, obj_id, len(obj_data["points"][frame_index])))
            
            if frame_targets:
                st.write(f"ğŸ“Š å½“å‰å¸§æœ‰{len(frame_targets)}ä¸ªç›®æ ‡æœ‰æ ‡æ³¨")
                for label_name, obj_id, point_count in frame_targets:
                    st.write(f"  â€¢ {label_name}-{obj_id}: {point_count}ä¸ªç‚¹")
                
                col_preview, col_save = st.columns(2)
                with col_preview:
                    if st.button("ğŸ‘ï¸ é¢„è§ˆå…¨éƒ¨ç›®æ ‡"):
                        preview_all_targets_only(frame_index, frame_np, frame_files, current_session_id)
                with col_save:
                    if st.button("ğŸ’¾ ä¿å­˜å…¨éƒ¨ç›®æ ‡"):
                        save_all_targets_in_frame(frame_index, frame_np, frame_files, current_session_id)
            else:
                st.info("ğŸ’¡ å½“å‰å¸§æ²¡æœ‰ç›®æ ‡æ ‡æ³¨ç‚¹")

            if st.button("ğŸ§¹ æ¸…é™¤å½“å‰å¸§æ‰€æœ‰ç›®æ ‡çš„ç‚¹"):
                # æ¸…é™¤å½“å‰å¸§çš„æ‰€æœ‰å¤šç›®æ ‡ç‚¹
                multi_target_data = st.session_state.get("multi_target_data", {})
                cleared_count = 0
                for label_name, label_data in multi_target_data.items():
                    for obj_id, obj_data in label_data["objects"].items():
                        if frame_index in obj_data["points"]:
                            obj_data["points"][frame_index] = []
                            cleared_count += 1
                st.success(f"âœ… å·²æ¸…é™¤å½“å‰å¸§ {cleared_count} ä¸ªç›®æ ‡çš„ç‚¹")
                st.session_state["refresh_flag"] = not st.session_state.get("refresh_flag", False)

            if st.button("ğŸ§¼ æ¸…é™¤æ‰€æœ‰ç›®æ ‡çš„ç‚¹"):
                # æ¸…é™¤æ‰€æœ‰å¸§çš„æ‰€æœ‰å¤šç›®æ ‡ç‚¹
                multi_target_data = st.session_state.get("multi_target_data", {})
                cleared_count = 0
                for label_name, label_data in multi_target_data.items():
                    for obj_id, obj_data in label_data["objects"].items():
                        obj_data["points"] = {}
                        obj_data["masks"] = {}
                        cleared_count += 1
                st.success(f"âœ… å·²æ¸…é™¤æ‰€æœ‰ {cleared_count} ä¸ªç›®æ ‡çš„ç‚¹å’Œæ©ç ")
                st.session_state["refresh_flag"] = not st.session_state.get("refresh_flag", False)




            
            # ===== å¤šç›®æ ‡100å¸§ç”ŸæˆåŠŸèƒ½ =====
            st.divider()
            st.subheader("ğŸš€ å¤šç›®æ ‡æ‰¹é‡ç”Ÿæˆ")
            
            # ç»Ÿè®¡æœ‰æ ‡æ³¨ç‚¹çš„ç›®æ ‡
            multi_target_data = st.session_state.get("multi_target_data", {})
            targets_with_annotations = 0
            target_info = []
            for label_name, label_data in multi_target_data.items():
                for obj_id, obj_data in label_data["objects"].items():
                    if any(obj_data["points"].values()):
                        targets_with_annotations += 1
                        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªæœ‰ç‚¹çš„å¸§ä½œä¸ºå‚è€ƒå¸§
                        ref_frame = None
                        for frame_idx, points in obj_data["points"].items():
                            if points:
                                ref_frame = frame_idx
                                break
                        if ref_frame is not None:
                            target_info.append((label_name, obj_id, ref_frame, len(obj_data["points"][ref_frame])))
            
            if targets_with_annotations > 0:
                st.write(f"ğŸ“Š å‘ç°{targets_with_annotations}ä¸ªç›®æ ‡æœ‰æ ‡æ³¨ç‚¹:")
                for label_name, obj_id, ref_frame, point_count in target_info:
                    st.write(f"  â€¢ {label_name}-{obj_id}: å‚è€ƒå¸§{ref_frame}, {point_count}ä¸ªç‚¹")
                
                if st.button("âš¡ ä¸€é”®ç”Ÿæˆå…¨éƒ¨ç›®æ ‡100å¸§æ©ç "):
                    with st.spinner(f"æ­£åœ¨ä¸º{targets_with_annotations}ä¸ªç›®æ ‡ç”Ÿæˆ100å¸§æ©ç ..."):
                        video_segments = generate_100_frames_all_targets()
                        if video_segments:
                            st.success(f"âœ… å…¨éƒ¨ç›®æ ‡100å¸§æ©ç ç”Ÿæˆå®Œæˆï¼")
                            st.balloons()
            else:
                st.info("ğŸ’¡ è¯·å…ˆä¸ºè‡³å°‘ä¸€ä¸ªç›®æ ‡æ·»åŠ æ ‡æ³¨ç‚¹")



    elif work_mode == "é¢„è§ˆ100å¸§æ©ç ":
        if "video_segments" not in st.session_state:
            st.warning("âš ï¸ è¯·å…ˆåœ¨'åˆå§‹æ ‡æ³¨'æ¨¡å¼ä¸‹ç”Ÿæˆ100å¸§æ©ç ")
        else:
            # æ£€æŸ¥æ˜¯å¦æœ‰å¤šç›®æ ‡å‚è€ƒä¿¡æ¯
            multi_target_ref = st.session_state.get("multi_target_reference", {})
            if multi_target_ref:
                st.subheader(f"ğŸï¸ å¤šç›®æ ‡100å¸§æ©ç é¢„è§ˆ ({len(multi_target_ref)}ä¸ªç›®æ ‡)")
                target_names = [f"ç›®æ ‡{obj_id}({info['label']})" for obj_id, info in multi_target_ref.items()]
                st.write("ğŸ¯ åŒ…å«ç›®æ ‡:", ", ".join(target_names))
            else:
                st.subheader("ğŸï¸ 100å¸§æ©ç é¢„è§ˆ")
            
            max_frames = min(len(frame_files), 100)
            preview_frame_idx = st.slider("é¢„è§ˆå¸§ä½ç½®", 0, max_frames-1, 0, key="preview_frame_idx")
            
            col1, col2 = st.columns([3, 1])
            
            with col1:
                # ç¡®ä¿æ¯æ¬¡éƒ½é‡æ–°åŠ è½½å¸§
                frame_path = os.path.join(FRAME_DIR, frame_files[preview_frame_idx])
                img = np.array(Image.open(frame_path).convert("RGB"))
                
                video_segments = st.session_state["video_segments"]
                frame_masks = video_segments.get(preview_frame_idx, {})
                
                # æ·»åŠ è°ƒè¯•ä¿¡æ¯
                st.write(f"ğŸ” å½“å‰é¢„è§ˆå¸§: {preview_frame_idx}, å¸§æ–‡ä»¶: {frame_files[preview_frame_idx]}")
                st.write(f"ğŸ” å‘ç°{len(frame_masks)}ä¸ªç›®æ ‡æ©ç ")
                for obj_id in frame_masks.keys():
                    mask_valid = frame_masks[obj_id] is not None and frame_masks[obj_id].sum() > 0
                    st.write(f"  â€¢ ç›®æ ‡{obj_id}: {'æœ‰æ•ˆ' if mask_valid else 'æ— æ•ˆ'}")
                
                # å¤šç›®æ ‡æ¨¡å¼æ˜¾ç¤º
                if len(frame_masks) > 0:
                    overlay = img.copy()
                    colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255), (0, 255, 255)]
                    valid_targets = 0
                    
                    # æ‰“å°è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºæ¯ä¸ªobj_idçš„æ˜ å°„
                    st.write(f"ğŸ” é¢„è§ˆè°ƒè¯• - å¸§{preview_frame_idx}çš„æ‰€æœ‰ç›®æ ‡:")
                    for obj_id, mask in frame_masks.items():
                        mask_size = mask.sum() if mask is not None else 0
                        st.write(f"  â€¢ obj_id={obj_id}: æ©ç åƒç´ æ•°={mask_size}")
                    
                    # æŒ‰obj_idæ’åºä»¥ç¡®ä¿ä¸€è‡´çš„é¢œè‰²åˆ†é…
                    sorted_targets = sorted(frame_masks.items(), key=lambda x: x[0])
                    
                    for i, (obj_id, mask) in enumerate(sorted_targets):
                        if mask is not None and mask.sum() > 0:
                            # ç¡®ä¿æ©ç æ ¼å¼æ­£ç¡®
                            if len(mask.shape) == 3:
                                mask = mask.squeeze()
                            if len(mask.shape) == 2 and mask.shape[:2] == img.shape[:2]:
                                mask = (mask > 0).astype(np.uint8)
                                
                                # ä¸ºæ¯ä¸ªç›®æ ‡ä½¿ç”¨ä¸åŒé¢œè‰²ï¼ˆåŸºäºobj_idç¡®ä¿ä¸€è‡´æ€§ï¼‰
                                target_color = colors[obj_id % len(colors)]
                                overlay[mask == 1] = (overlay[mask == 1] * 0.7 + np.array(target_color) * 0.3).astype(np.uint8)
                                
                                # ç»˜åˆ¶è¾¹ç•Œæ¡†
                                bbox_method = st.session_state.get("bbox_method", "ä¼ ç»Ÿæ–¹æ³•")
                                box = get_bbox_by_method(mask, bbox_method)
                                
                                if box is not None:
                                    x1, y1, x2, y2 = box
                                    cv2.rectangle(overlay, (x1, y1), (x2, y2), target_color, 2)
                                    # æ·»åŠ æ ‡ç­¾ï¼ˆä½¿ç”¨ann_obj_idï¼‰
                                    if multi_target_ref and obj_id in multi_target_ref:
                                        label_name = multi_target_ref[obj_id]["label"]
                                        cv2.putText(overlay, f"{label_name}-{obj_id}", (x1, y1-10), 
                                                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, target_color, 2)
                                    else:
                                        cv2.putText(overlay, f"ann_obj_id{obj_id}", (x1, y1-10), 
                                                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, target_color, 2)
                                
                                valid_targets += 1
                                st.write(f"  âœ… æˆåŠŸæ¸²æŸ“obj_id={obj_id}, é¢œè‰²ç´¢å¼•={obj_id % len(colors)}")
                            else:
                                st.warning(f"  âš ï¸ obj_id={obj_id}: æ©ç å°ºå¯¸ä¸åŒ¹é… - æ©ç :{mask.shape}, å›¾åƒ:{img.shape[:2]}")
                        else:
                            st.info(f"  â„¹ï¸ obj_id={obj_id}: æ©ç ä¸ºç©ºæˆ–æ— æ•ˆ")
                    
                    if valid_targets > 0:
                        caption = f"å¸§ {preview_frame_idx} - å¤šç›®æ ‡é¢„è§ˆ ({valid_targets}ä¸ªç›®æ ‡)" if len(frame_masks) > 1 else f"å¸§ {preview_frame_idx} - ç›®æ ‡é¢„è§ˆ"
                        st.image(overlay, caption=caption)
                    else:
                        st.image(img, caption=f"å¸§ {preview_frame_idx} - æ— æœ‰æ•ˆæ©ç ")
                else:
                    st.image(img, caption=f"å¸§ {preview_frame_idx} - æ— æ©ç ")
            
            with col2:
                st.markdown("### ğŸ”§ æ“ä½œé€‰é¡¹")
                if st.button("é€‰æ‹©æ­¤å¸§è¿›è¡Œä¿®æ­£"):
                    st.session_state["refine_frame_idx"] = preview_frame_idx
                    st.success(f"âœ… å·²é€‰æ‹©å¸§ {preview_frame_idx} è¿›è¡Œä¿®æ­£")
                
                if st.button("ğŸ“¤ æ‰¹é‡æ·»åŠ åˆ°ç»Ÿä¸€æ•°æ®é›†"):
                    # æ£€æŸ¥æ˜¯å¦æœ‰å¤šç›®æ ‡å‚è€ƒä¿¡æ¯
                    multi_target_ref = st.session_state.get("multi_target_reference", {})
                    
                    if multi_target_ref:
                        # å¤šç›®æ ‡æ¨¡å¼ï¼šæ‰¹é‡æ·»åŠ æ‰€æœ‰ç›®æ ‡
                        all_labels = list(st.session_state.get("multi_target_data", {}).keys())
                        exported_count = 0
                        
                        for i in range(max_frames):
                            frame_masks = video_segments.get(i, {})
                            
                            for ann_obj_id, mask in frame_masks.items():
                                if mask is not None and mask.sum() > 0:
                                    # ä½¿ç”¨è¾¹ç•Œæ¡†ç®—æ³•
                                    bbox_method = st.session_state.get("bbox_method", "ä¼ ç»Ÿæ–¹æ³•")
                                    box = get_bbox_by_method(mask, bbox_method)
                                    
                                    if box is not None:
                                        x1, y1, x2, y2 = box
                                        # ç¡®ä¿æ©ç æ˜¯2Dçš„
                                        if len(mask.shape) == 3:
                                            mask = mask.squeeze()
                                        if len(mask.shape) == 2:
                                            h, w = mask.shape
                                        else:
                                            h, w = img.shape[:2]
                                        
                                        # è·å–å¯¹åº”çš„æ ‡ç­¾ï¼ˆä½¿ç”¨ann_obj_idï¼‰
                                        if ann_obj_id in multi_target_ref:
                                            label_name = multi_target_ref[ann_obj_id]["label"]
                                            label_id = all_labels.index(label_name)
                                            yolo_line = f"{label_id} {(x1+x2)/2/w:.6f} {(y1+y2)/2/h:.6f} {(x2-x1)/w:.6f} {(y2-y1)/h:.6f}\n"
                                            
                                            # ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶åï¼ˆä½¿ç”¨ann_obj_idï¼‰
                                            base_name = frame_files[i].replace(".jpg", "")
                                            unique_name = f"{current_session_id}_{base_name}_ann{ann_obj_id}"
                                            
                                            # ä¿å­˜å›¾åƒåˆ°ç»Ÿä¸€ç›®å½•
                                            src_img_path = os.path.join(FRAME_DIR, frame_files[i])
                                            dst_img_path = os.path.join(UNIFIED_FRAME_DIR, f"{unique_name}.jpg")
                                            shutil.copy2(src_img_path, dst_img_path)
                                            
                                            # ä¿å­˜æ ‡æ³¨åˆ°ç»Ÿä¸€ç›®å½•
                                            label_file = os.path.join(UNIFIED_LABEL_DIR, f"{unique_name}.txt")
                                            with open(label_file, "w") as f:
                                                f.write(yolo_line)
                                            
                                            exported_count += 1
                                        else:
                                            st.warning(f"âš ï¸ æœªæ‰¾åˆ°ann_obj_id={ann_obj_id}çš„æ ‡ç­¾æ˜ å°„")
                        
                        # æ›´æ–°ç±»åˆ«æ–‡ä»¶
                        if exported_count > 0:
                            classes_file = os.path.join(UNIFIED_LABEL_DIR, "classes.txt")
                            with open(classes_file, "w") as f:
                                for label_name in all_labels:
                                    f.write(f"{label_name}\n")
                        
                        st.success(f"âœ… å¤šç›®æ ‡æ¨¡å¼ï¼šå·²æ·»åŠ  {exported_count} ä¸ªç›®æ ‡æ•°æ®åˆ°ç»Ÿä¸€æ•°æ®é›†")
                    else:
                        st.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°å¤šç›®æ ‡å‚è€ƒä¿¡æ¯ï¼Œè¯·ä½¿ç”¨å¤šç›®æ ‡æ¨¡å¼ç”Ÿæˆ100å¸§æ©ç ")

    elif work_mode == "ä¿®æ­£ç‰¹å®šå¸§":
        if "refine_frame_idx" not in st.session_state:
            st.warning("âš ï¸ è¯·å…ˆåœ¨'é¢„è§ˆ100å¸§æ©ç 'æ¨¡å¼ä¸‹é€‰æ‹©è¦ä¿®æ­£çš„å¸§")
        else:
            refine_frame_idx = st.session_state["refine_frame_idx"]
            st.subheader(f"ğŸ”§ ä¿®æ­£å¸§ {refine_frame_idx}")
            
            col1, col2 = st.columns([3, 1])
            
            with col1:
                frame_path = os.path.join(FRAME_DIR, frame_files[refine_frame_idx])
                img = np.array(Image.open(frame_path).convert("RGB"))
                
                # æ˜¾ç¤ºå½“å‰æ©ç 
                video_segments = st.session_state.get("video_segments", {})
                mask = video_segments.get(refine_frame_idx, {}).get(1, None)  # ä½¿ç”¨obj_id=1
                
                if mask is not None and mask.sum() > 0:
                    overlay = img.copy()
                    
                    # ç¡®ä¿æ©ç å½¢çŠ¶ä¸å›¾åƒåŒ¹é…
                    if len(mask.shape) == 3:
                        mask = mask.squeeze()
                    if len(mask.shape) == 1:
                        # å¦‚æœæ©ç æ˜¯1Dï¼Œéœ€è¦é‡æ–°reshape
                        h, w = img.shape[:2]
                        if mask.size == h * w:
                            mask = mask.reshape(h, w)
                        else:
                            st.error(f"æ©ç å¤§å°ä¸åŒ¹é…: æ©ç å¤§å°={mask.size}, å›¾åƒå¤§å°={h}x{w}")
                            st.image(img, caption=f"å¸§ {refine_frame_idx} - æ©ç å½¢çŠ¶é”™è¯¯")
                    
                    # ç¡®ä¿æ©ç æ˜¯äºŒå€¼çš„
                    mask = (mask > 0).astype(np.uint8)
                    
                    overlay[mask == 1] = (overlay[mask == 1] * 0.5 + np.array([128,128,255]) * 0.5).astype(np.uint8)
                    
                    # æ˜¾ç¤ºä¿®æ­£ç‚¹
                    refine_points = st.session_state.get("refine_points", [])
                    for x, y, l in refine_points:
                        color = (0, 255, 0) if l == 1 else (0, 0, 255)
                        cv2.circle(overlay, (int(x), int(y)), 8, color, -1)
                    
                    click = streamlit_image_coordinates(overlay, key=f"refine_{refine_frame_idx}")
                    if click:
                        st.session_state["last_refine_click"] = (click["x"], click["y"])
                    
                    st.image(overlay, caption=f"ä¿®æ­£å¸§ {refine_frame_idx} - ç‚¹å‡»æ·»åŠ ä¿®æ­£ç‚¹")
                else:
                    st.image(img, caption=f"å¸§ {refine_frame_idx} - æ— æ©ç ")
            
            with col2:
                st.markdown("### ğŸ”§ ä¿®æ­£æ“ä½œ")
                
                if "last_refine_click" in st.session_state:
                    col_a, col_b = st.columns(2)
                    with col_a:
                        if st.button("æ·»åŠ æ­£ç‚¹ä¿®æ­£"):
                            if "refine_points" not in st.session_state:
                                st.session_state["refine_points"] = []
                            st.session_state["refine_points"].append((*st.session_state["last_refine_click"], 1))
                            del st.session_state["last_refine_click"]
                            st.rerun()
                    
                    with col_b:
                        if st.button("æ·»åŠ è´Ÿç‚¹ä¿®æ­£"):
                            if "refine_points" not in st.session_state:
                                st.session_state["refine_points"] = []
                            st.session_state["refine_points"].append((*st.session_state["last_refine_click"], 0))
                            del st.session_state["last_refine_click"]
                            st.rerun()
                
                if st.button("åº”ç”¨ä¿®æ­£å¹¶é‡æ–°ä¼ æ’­"):
                    refine_points = st.session_state.get("refine_points", [])
                    if refine_points:
                        with st.spinner("æ­£åœ¨åº”ç”¨ä¿®æ­£å¹¶é‡æ–°ä¼ æ’­..."):
                            with torch.autocast(get_autocast_device()):
                                inference_state = st.session_state["inference_state"]
                                pts = [[p[0], p[1]] for p in refine_points]
                                lbls = [p[2] for p in refine_points]
                                
                                # åœ¨æŒ‡å®šå¸§æ·»åŠ ä¿®æ­£ç‚¹
                                sam2_model.add_new_points_or_box(
                                    inference_state=inference_state,
                                    frame_idx=refine_frame_idx,
                                    obj_id=1,  # ä½¿ç”¨obj_id=1
                                    points=pts,
                                    labels=lbls,
                                )
                                
                                # é‡æ–°ä¼ æ’­
                                video_segments = {}
                                for i, obj_ids, mask_logits in sam2_model.propagate_in_video(inference_state):
                                    video_segments[i] = {
                                        obj_id: (mask_logits[j] > 0).cpu().numpy()
                                        for j, obj_id in enumerate(obj_ids)
                                    }
                                    # é™åˆ¶åªå¤„ç†å‰100å¸§
                                    if i >= 99:
                                        break
                                
                                st.session_state["video_segments"] = video_segments
                                st.session_state["refine_points"] = []
                        st.success("âœ… ä¿®æ­£å®Œæˆå¹¶é‡æ–°ä¼ æ’­ï¼")
                    else:
                        st.warning("âš ï¸ è¯·å…ˆæ·»åŠ ä¿®æ­£ç‚¹")
                
                if st.button("æ¸…é™¤ä¿®æ­£ç‚¹"):
                    st.session_state["refine_points"] = []
                    st.success("âœ… ä¿®æ­£ç‚¹å·²æ¸…é™¤")
                    st.rerun()

# ä¾§è¾¹æ æ˜¾ç¤ºå½“å‰çŠ¶æ€
with st.sidebar:
    st.header("ğŸ“Š å½“å‰çŠ¶æ€")
    if current_session_id:
        st.write(f"ä¼šè¯ID: {current_session_id}")
        device = get_device()
        if device.type == "mps":
            st.write("è®¾å¤‡: ğŸš€ Apple Silicon MPS")
        elif device.type == "cuda":
            st.write("è®¾å¤‡: ğŸš€ CUDA GPU")
        else:
            st.write("è®¾å¤‡: ğŸ’» CPU")
        
        # GPUå†…å­˜ç›‘æ§
        if device.type == "cuda":
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            gpu_allocated = torch.cuda.memory_allocated(0) / 1024**3
            gpu_cached = torch.cuda.memory_reserved(0) / 1024**3
            st.write(f"GPUæ€»å†…å­˜: {gpu_memory:.1f} GB")
            st.write(f"GPUå·²åˆ†é…: {gpu_allocated:.1f} GB")
            st.write(f"GPUç¼“å­˜: {gpu_cached:.1f} GB")
            
            if st.button("ğŸ§¹ æ¸…ç†GPUå†…å­˜"):
                torch.cuda.empty_cache()
                st.success("âœ… GPUå†…å­˜å·²æ¸…ç†")
                st.rerun()
        
        # MPSå†…å­˜ç›‘æ§ï¼ˆApple Siliconï¼‰
        elif device.type == "mps":
            if st.button("ğŸ§¹ æ¸…ç†MPSå†…å­˜"):
                torch.mps.empty_cache()
                st.success("âœ… MPSå†…å­˜å·²æ¸…ç†")
                st.rerun()
        
        if "video_segments" in st.session_state:
            total_masks = sum(1 for frame_data in st.session_state["video_segments"].values() 
                            for mask in frame_data.values() if mask.sum() > 0)
            st.write(f"æœ‰æ•ˆæ©ç æ•°: {total_masks}")
        
        # å¤šç›®æ ‡æ•°æ®ç»Ÿè®¡
        multi_target_data = st.session_state.get("multi_target_data", {})
        if multi_target_data:
            st.write("å¤šç›®æ ‡ç»Ÿè®¡:")
            total_labels = len(multi_target_data)
            total_objects = sum(len(label_data["objects"]) for label_data in multi_target_data.values())
            total_points = 0
            total_masks = 0
            for label_data in multi_target_data.values():
                for obj_data in label_data["objects"].values():
                    total_points += sum(len(points) for points in obj_data["points"].values())
                    total_masks += len(obj_data["masks"])
            
            st.write(f"æ ‡ç­¾æ•°é‡: {total_labels}")
            st.write(f"ç›®æ ‡æ•°é‡: {total_objects}")
            st.write(f"æ ‡æ³¨ç‚¹æ•°: {total_points}")
            st.write(f"ç”Ÿæˆæ©ç : {total_masks}")
    
    # è®¾ç½®GPUå†…å­˜ç®¡ç†
    st.header("âš™ï¸ å†…å­˜è®¾ç½®")
    if st.button("è®¾ç½®GPUå†…å­˜ä¼˜åŒ–"):
        os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
        st.success("âœ… å·²è®¾ç½®GPUå†…å­˜ä¼˜åŒ–")
        st.write("é‡å¯åº”ç”¨ç”Ÿæ•ˆ") 