#!/usr/bin/env python3
"""
ç»Ÿä¸€æ•°æ®é›†è½¬æ¢è„šæœ¬
å°†streamlit_app_enhanced.pyç”Ÿæˆçš„ç»Ÿä¸€æ•°æ®é›†è½¬æ¢ä¸ºYOLOè®­ç»ƒæ ¼å¼
"""

import os
import shutil
import random
from pathlib import Path
import yaml

def convert_unified_dataset(
    dataset_name="lajiao_dataset",
    output_dir="yolo_dataset",
    train_ratio=0.8,
    val_ratio=0.2
):
    """
    å°†ç»Ÿä¸€æ•°æ®é›†è½¬æ¢ä¸ºYOLOè®­ç»ƒæ ¼å¼
    
    Args:
        dataset_name: ç»Ÿä¸€æ•°æ®é›†åç§°
        output_dir: è¾“å‡ºYOLOæ•°æ®é›†ç›®å½•
        train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
        val_ratio: éªŒè¯é›†æ¯”ä¾‹
    """
    
    print(f"ğŸ”„ è½¬æ¢ç»Ÿä¸€æ•°æ®é›†: {dataset_name}")
    
    # ç»Ÿä¸€æ•°æ®é›†è·¯å¾„
    unified_frame_dir = f"frames_{dataset_name}"
    unified_label_dir = f"labels_{dataset_name}"
    
    # æ£€æŸ¥ç»Ÿä¸€æ•°æ®é›†æ˜¯å¦å­˜åœ¨
    if not os.path.exists(unified_frame_dir):
        print(f"âŒ å›¾åƒç›®å½•ä¸å­˜åœ¨: {unified_frame_dir}")
        return False
    
    if not os.path.exists(unified_label_dir):
        print(f"âŒ æ ‡æ³¨ç›®å½•ä¸å­˜åœ¨: {unified_label_dir}")
        return False
    
    # åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    # åˆ›å»ºå­ç›®å½•
    for split in ['train', 'val']:
        (output_path / split / 'images').mkdir(parents=True, exist_ok=True)
        (output_path / split / 'labels').mkdir(parents=True, exist_ok=True)
    
    # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
    image_files = [f for f in os.listdir(unified_frame_dir) if f.endswith('.jpg')]
    
    # è¿‡æ»¤æœ‰å¯¹åº”æ ‡æ³¨æ–‡ä»¶çš„å›¾åƒ
    valid_files = []
    for img_file in image_files:
        label_file = os.path.join(unified_label_dir, img_file.replace('.jpg', '.txt'))
        if os.path.exists(label_file):
            # æ£€æŸ¥æ ‡æ³¨æ–‡ä»¶æ˜¯å¦ä¸ºç©º
            if os.path.getsize(label_file) > 0:
                valid_files.append(img_file.replace('.jpg', ''))
    
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   æ€»å›¾åƒæ–‡ä»¶: {len(image_files)}")
    print(f"   æœ‰æ•ˆå›¾åƒ-æ ‡æ³¨å¯¹: {len(valid_files)}")
    
    if len(valid_files) == 0:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„å›¾åƒ-æ ‡æ³¨å¯¹")
        return False
    
    # éšæœºæ‰“ä¹±æ–‡ä»¶åˆ—è¡¨
    random.shuffle(valid_files)
    
    # è®¡ç®—åˆ†å‰²ç‚¹
    train_count = int(len(valid_files) * train_ratio)
    val_count = len(valid_files) - train_count
    
    train_files = valid_files[:train_count]
    val_files = valid_files[train_count:]
    
    print(f"ğŸ“ˆ æ•°æ®åˆ†å‰²:")
    print(f"   è®­ç»ƒé›†: {len(train_files)} ä¸ªæ–‡ä»¶")
    print(f"   éªŒè¯é›†: {len(val_files)} ä¸ªæ–‡ä»¶")
    
    # å¤åˆ¶æ–‡ä»¶åˆ°å¯¹åº”ç›®å½•
    def copy_files(file_list, split):
        copied_count = 0
        for filename in file_list:
            # å¤åˆ¶å›¾åƒæ–‡ä»¶
            src_img = os.path.join(unified_frame_dir, f"{filename}.jpg")
            dst_img = output_path / split / 'images' / f"{filename}.jpg"
            if os.path.exists(src_img):
                shutil.copy2(src_img, dst_img)
                copied_count += 1
            
            # å¤åˆ¶æ ‡æ³¨æ–‡ä»¶
            src_label = os.path.join(unified_label_dir, f"{filename}.txt")
            dst_label = output_path / split / 'labels' / f"{filename}.txt"
            if os.path.exists(src_label):
                shutil.copy2(src_label, dst_label)
        
        print(f"   {split}: å¤åˆ¶äº† {copied_count} ä¸ªæ–‡ä»¶")
        return copied_count
    
    train_copied = copy_files(train_files, 'train')
    val_copied = copy_files(val_files, 'val')
    
    # è¯»å–ç±»åˆ«ä¿¡æ¯
    classes_file = os.path.join(unified_label_dir, "classes.txt")
    if os.path.exists(classes_file):
        with open(classes_file, 'r', encoding='utf-8') as f:
            classes = [line.strip() for line in f.readlines() if line.strip()]
    else:
        classes = ['lajiao']  # é»˜è®¤ç±»åˆ«
        print("âš ï¸ æœªæ‰¾åˆ°classes.txtï¼Œä½¿ç”¨é»˜è®¤ç±»åˆ«")
    
    # åˆ›å»ºYOLOé…ç½®æ–‡ä»¶
    config = {
        'path': str(output_path.absolute()),  # æ•°æ®é›†æ ¹ç›®å½•
        'train': 'train/images',  # è®­ç»ƒå›¾åƒç›®å½•ï¼ˆç›¸å¯¹è·¯å¾„ï¼‰
        'val': 'val/images',      # éªŒè¯å›¾åƒç›®å½•ï¼ˆç›¸å¯¹è·¯å¾„ï¼‰
        'nc': len(classes),       # ç±»åˆ«æ•°é‡
        'names': classes          # ç±»åˆ«åç§°åˆ—è¡¨
    }
    
    config_file = output_path / "data.yaml"
    with open(config_file, 'w', encoding='utf-8') as f:
        yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
    
    print(f"\nâœ… æ•°æ®é›†è½¬æ¢å®Œæˆï¼")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_path.absolute()}")
    print(f"ğŸ“„ é…ç½®æ–‡ä»¶: {config_file.absolute()}")
    print(f"ğŸ·ï¸ ç±»åˆ«æ•°é‡: {len(classes)}")
    print(f"ğŸ·ï¸ ç±»åˆ«åˆ—è¡¨: {classes}")
    print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
    print(f"   è®­ç»ƒé›†: {train_copied} å¼ å›¾åƒ")
    print(f"   éªŒè¯é›†: {val_copied} å¼ å›¾åƒ")
    print(f"   æ€»è®¡: {train_copied + val_copied} å¼ å›¾åƒ")
    
    return True

def list_available_datasets():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ç»Ÿä¸€æ•°æ®é›†"""
    print("ğŸ“‹ å¯ç”¨çš„ç»Ÿä¸€æ•°æ®é›†:")
    
    datasets = []
    for item in os.listdir('.'):
        if os.path.isdir(item) and item.startswith('frames_'):
            dataset_name = item.replace('frames_', '')
            label_dir = f"labels_{dataset_name}"
            if os.path.exists(label_dir):
                # ç»Ÿè®¡æ–‡ä»¶æ•°é‡
                frame_count = len([f for f in os.listdir(item) if f.endswith('.jpg')])
                label_count = len([f for f in os.listdir(label_dir) if f.endswith('.txt')])
                datasets.append((dataset_name, frame_count, label_count))
                print(f"   ğŸ“ {dataset_name}: {frame_count} å›¾åƒ, {label_count} æ ‡æ³¨")
    
    if not datasets:
        print("   âŒ æœªæ‰¾åˆ°ä»»ä½•ç»Ÿä¸€æ•°æ®é›†")
    
    return datasets

def merge_datasets(dataset_names, merged_name="merged_dataset"):
    """åˆå¹¶å¤šä¸ªç»Ÿä¸€æ•°æ®é›†"""
    print(f"ğŸ”— åˆå¹¶æ•°æ®é›†åˆ°: {merged_name}")
    
    merged_frame_dir = f"frames_{merged_name}"
    merged_label_dir = f"labels_{merged_name}"
    
    os.makedirs(merged_frame_dir, exist_ok=True)
    os.makedirs(merged_label_dir, exist_ok=True)
    
    all_classes = set()
    total_files = 0
    
    for dataset_name in dataset_names:
        frame_dir = f"frames_{dataset_name}"
        label_dir = f"labels_{dataset_name}"
        
        if not os.path.exists(frame_dir) or not os.path.exists(label_dir):
            print(f"âš ï¸ è·³è¿‡ä¸å­˜åœ¨çš„æ•°æ®é›†: {dataset_name}")
            continue
        
        print(f"   ğŸ“‚ å¤„ç†æ•°æ®é›†: {dataset_name}")
        
        # å¤åˆ¶å›¾åƒæ–‡ä»¶
        for img_file in os.listdir(frame_dir):
            if img_file.endswith('.jpg'):
                src_path = os.path.join(frame_dir, img_file)
                # æ·»åŠ æ•°æ®é›†å‰ç¼€é¿å…å†²çª
                dst_name = f"{dataset_name}_{img_file}"
                dst_path = os.path.join(merged_frame_dir, dst_name)
                shutil.copy2(src_path, dst_path)
        
        # å¤åˆ¶æ ‡æ³¨æ–‡ä»¶
        for label_file in os.listdir(label_dir):
            if label_file.endswith('.txt') and label_file != 'classes.txt':
                src_path = os.path.join(label_dir, label_file)
                # æ·»åŠ æ•°æ®é›†å‰ç¼€é¿å…å†²çª
                dst_name = f"{dataset_name}_{label_file}"
                dst_path = os.path.join(merged_label_dir, dst_name)
                shutil.copy2(src_path, dst_path)
                total_files += 1
        
        # æ”¶é›†ç±»åˆ«ä¿¡æ¯
        classes_file = os.path.join(label_dir, "classes.txt")
        if os.path.exists(classes_file):
            with open(classes_file, 'r', encoding='utf-8') as f:
                classes = [line.strip() for line in f.readlines() if line.strip()]
                all_classes.update(classes)
    
    # ä¿å­˜åˆå¹¶åçš„ç±»åˆ«æ–‡ä»¶
    merged_classes_file = os.path.join(merged_label_dir, "classes.txt")
    with open(merged_classes_file, 'w', encoding='utf-8') as f:
        for class_name in sorted(all_classes):
            f.write(f"{class_name}\n")
    
    print(f"âœ… æ•°æ®é›†åˆå¹¶å®Œæˆ!")
    print(f"   ğŸ“ åˆå¹¶åç›®å½•: {merged_frame_dir}, {merged_label_dir}")
    print(f"   ğŸ“Š æ€»æ–‡ä»¶æ•°: {total_files}")
    print(f"   ğŸ·ï¸ ç±»åˆ«æ•°: {len(all_classes)}")
    print(f"   ğŸ·ï¸ ç±»åˆ«: {sorted(all_classes)}")
    
    return merged_name

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ç»Ÿä¸€æ•°æ®é›†è½¬æ¢å·¥å…·")
    parser.add_argument("--list", action="store_true", help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ•°æ®é›†")
    parser.add_argument("--convert", type=str, help="è½¬æ¢æŒ‡å®šæ•°æ®é›†")
    parser.add_argument("--merge", nargs="+", help="åˆå¹¶å¤šä¸ªæ•°æ®é›†")
    parser.add_argument("--output", type=str, default="yolo_dataset", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--train-ratio", type=float, default=0.8, help="è®­ç»ƒé›†æ¯”ä¾‹")
    
    args = parser.parse_args()
    
    if args.list:
        list_available_datasets()
    elif args.convert:
        success = convert_unified_dataset(
            dataset_name=args.convert,
            output_dir=args.output,
            train_ratio=args.train_ratio
        )
        if success:
            print(f"\nğŸš€ å¯ä»¥å¼€å§‹è®­ç»ƒäº†ï¼")
            print(f"ä½¿ç”¨å‘½ä»¤: python train_yolo11.py")
    elif args.merge:
        merged_name = merge_datasets(args.merge)
        print(f"\nğŸ’¡ æ¥ä¸‹æ¥å¯ä»¥è½¬æ¢åˆå¹¶åçš„æ•°æ®é›†:")
        print(f"python {__file__} --convert {merged_name}")
    else:
        # é»˜è®¤è¡Œä¸ºï¼šåˆ—å‡ºæ•°æ®é›†å¹¶è½¬æ¢ç¬¬ä¸€ä¸ª
        datasets = list_available_datasets()
        if datasets:
            dataset_name = datasets[0][0]
            print(f"\nğŸ¯ è‡ªåŠ¨è½¬æ¢æ•°æ®é›†: {dataset_name}")
            success = convert_unified_dataset(dataset_name)
            if success:
                print(f"\nğŸš€ å¯ä»¥å¼€å§‹è®­ç»ƒäº†ï¼")
                print(f"ä½¿ç”¨å‘½ä»¤: python train_yolo11.py")
        else:
            print("\nâŒ æ²¡æœ‰æ‰¾åˆ°å¯è½¬æ¢çš„æ•°æ®é›†")
            print("ğŸ’¡ è¯·å…ˆä½¿ç”¨streamlit_app_enhanced.pyç”Ÿæˆç»Ÿä¸€æ•°æ®é›†") 