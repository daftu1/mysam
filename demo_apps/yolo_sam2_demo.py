#!/usr/bin/env python3
"""
YOLO-SAM2 å‘½ä»¤è¡Œæ¼”ç¤ºè„šæœ¬
å±•ç¤ºå¦‚ä½•åœ¨å‘½ä»¤è¡Œä¸­ä½¿ç”¨YOLO+SAM2è¿›è¡Œè§†é¢‘åˆ†å‰²
"""

import cv2
import numpy as np
import torch
import os
import argparse
from ultralytics import YOLO
import sys

# æ·»åŠ SAM2è·¯å¾„
sys.path.append('/home/zcx/sam2')
from sam2.build_sam import build_sam2_video_predictor

def load_models(yolo_path, sam2_checkpoint):
    """åŠ è½½YOLOå’ŒSAM2æ¨¡å‹"""
    print("ğŸ”„ åŠ è½½æ¨¡å‹...")
    
    # åŠ è½½YOLO
    yolo_model = YOLO(yolo_path)
    print(f"âœ… YOLOæ¨¡å‹åŠ è½½å®Œæˆ: {yolo_path}")
    
    # åŠ è½½SAM2
    model_cfg = "configs/sam2.1/sam2.1_hiera_b+.yaml"
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    sam2_model = build_sam2_video_predictor(model_cfg, sam2_checkpoint, device=device)
    print(f"âœ… SAM2æ¨¡å‹åŠ è½½å®Œæˆ: {sam2_checkpoint}")
    
    return yolo_model, sam2_model

def extract_frames(video_path, output_dir, max_frames=50):
    """æå–è§†é¢‘å¸§"""
    print(f"ğŸ¬ æå–è§†é¢‘å¸§: {video_path}")
    
    os.makedirs(output_dir, exist_ok=True)
    
    cap = cv2.VideoCapture(video_path)
    frame_count = 0
    
    while frame_count < max_frames:
        ret, frame = cap.read()
        if not ret:
            break
        
        frame_path = os.path.join(output_dir, f"{frame_count:04d}.jpg")
        cv2.imwrite(frame_path, frame)
        frame_count += 1
    
    cap.release()
    print(f"âœ… æå–äº† {frame_count} å¸§åˆ° {output_dir}")
    return frame_count

def yolo_detect(yolo_model, image_path, conf_threshold=0.25):
    """YOLOç›®æ ‡æ£€æµ‹"""
    results = yolo_model.predict(image_path, conf=conf_threshold, verbose=False)
    
    detections = []
    if len(results) > 0 and len(results[0].boxes) > 0:
        boxes = results[0].boxes
        for box in boxes:
            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
            conf = box.conf[0].cpu().numpy()
            cls = int(box.cls[0].cpu().numpy())
            
            detections.append({
                'bbox': [int(x1), int(y1), int(x2), int(y2)],
                'confidence': float(conf),
                'class': cls,
                'center': [int((x1+x2)/2), int((y1+y2)/2)]
            })
    
    return detections

def sam2_segment(sam2_model, frame_dir, detections, reference_frame=0):
    """SAM2è§†é¢‘åˆ†å‰²"""
    print(f"ğŸ¨ å¼€å§‹SAM2åˆ†å‰²ï¼Œå‚è€ƒå¸§: {reference_frame}")
    
    # åˆå§‹åŒ–æ¨ç†çŠ¶æ€
    inference_state = sam2_model.init_state(video_path=frame_dir)
    
    # ä¸ºæ¯ä¸ªæ£€æµ‹ç›®æ ‡æ·»åŠ ç‚¹
    for obj_id, detection in enumerate(detections, 1):
        center_x, center_y = detection['center']
        points = np.array([[center_x, center_y]], dtype=np.float32)
        labels = np.array([1], dtype=np.int32)
        
        sam2_model.add_new_points_or_box(
            inference_state=inference_state,
            frame_idx=reference_frame,
            obj_id=obj_id,
            points=points,
            labels=labels,
        )
        print(f"   æ·»åŠ ç›®æ ‡ {obj_id}: ä¸­å¿ƒç‚¹ ({center_x}, {center_y})")
    
    # ä¼ æ’­åˆ†å‰²
    video_segments = {}
    for out_frame_idx, out_obj_ids, out_mask_logits in sam2_model.propagate_in_video(inference_state):
        video_segments[out_frame_idx] = {}
        for i, obj_id in enumerate(out_obj_ids):
            mask = (out_mask_logits[i] > 0.0).cpu().numpy().squeeze()
            video_segments[out_frame_idx][obj_id] = mask
    
    print(f"âœ… åˆ†å‰²å®Œæˆï¼Œå¤„ç†äº† {len(video_segments)} å¸§")
    return video_segments

def save_results(frame_dir, video_segments, detections, output_dir):
    """ä¿å­˜åˆ†å‰²ç»“æœ"""
    print(f"ğŸ’¾ ä¿å­˜ç»“æœåˆ°: {output_dir}")
    
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(os.path.join(output_dir, "masks"), exist_ok=True)
    os.makedirs(os.path.join(output_dir, "visualizations"), exist_ok=True)
    
    # é¢œè‰²åˆ—è¡¨
    colors = [
        (255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0),
        (255, 0, 255), (0, 255, 255), (128, 0, 0), (0, 128, 0)
    ]
    
    saved_count = 0
    
    for frame_idx, masks in video_segments.items():
        # è¯»å–åŸå§‹å¸§
        frame_path = os.path.join(frame_dir, f"{frame_idx:04d}.jpg")
        if not os.path.exists(frame_path):
            continue
        
        image = cv2.imread(frame_path)
        overlay = image.copy()
        
        # ä¿å­˜æ¯ä¸ªç›®æ ‡çš„æ©ç 
        for obj_id, mask in masks.items():
            if mask.sum() > 0:
                # ä¿å­˜æ©ç 
                mask_path = os.path.join(output_dir, "masks", f"frame_{frame_idx:04d}_obj_{obj_id}.png")
                mask_image = (mask * 255).astype(np.uint8)
                cv2.imwrite(mask_path, mask_image)
                
                # åœ¨å¯è§†åŒ–å›¾åƒä¸Šæ·»åŠ æ©ç 
                color = colors[(obj_id-1) % len(colors)]
                overlay[mask > 0] = (overlay[mask > 0] * 0.6 + np.array(color) * 0.4).astype(np.uint8)
                saved_count += 1
        
        # æ·»åŠ æ£€æµ‹æ¡†
        for i, detection in enumerate(detections):
            x1, y1, x2, y2 = detection['bbox']
            cv2.rectangle(overlay, (x1, y1), (x2, y2), (0, 255, 0), 2)
            cv2.putText(overlay, f"Obj {i+1}", (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
        
        # ä¿å­˜å¯è§†åŒ–ç»“æœ
        vis_path = os.path.join(output_dir, "visualizations", f"frame_{frame_idx:04d}.jpg")
        cv2.imwrite(vis_path, overlay)
    
    print(f"âœ… ä¿å­˜äº† {saved_count} ä¸ªæ©ç å’Œ {len(video_segments)} ä¸ªå¯è§†åŒ–å›¾åƒ")

def create_video(output_dir, output_video, fps=10):
    """åˆ›å»ºåˆ†å‰²ç»“æœè§†é¢‘"""
    print(f"ğŸ¬ åˆ›å»ºåˆ†å‰²è§†é¢‘: {output_video}")
    
    vis_dir = os.path.join(output_dir, "visualizations")
    if not os.path.exists(vis_dir):
        print("âŒ å¯è§†åŒ–ç›®å½•ä¸å­˜åœ¨")
        return
    
    # è·å–æ‰€æœ‰å¯è§†åŒ–å›¾åƒ
    vis_files = sorted([f for f in os.listdir(vis_dir) if f.endswith('.jpg')])
    if not vis_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯è§†åŒ–å›¾åƒ")
        return
    
    # è¯»å–ç¬¬ä¸€å¼ å›¾åƒè·å–å°ºå¯¸
    first_image = cv2.imread(os.path.join(vis_dir, vis_files[0]))
    height, width = first_image.shape[:2]
    
    # åˆ›å»ºè§†é¢‘å†™å…¥å™¨
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_video, fourcc, fps, (width, height))
    
    for vis_file in vis_files:
        frame = cv2.imread(os.path.join(vis_dir, vis_file))
        out.write(frame)
    
    out.release()
    print(f"âœ… è§†é¢‘åˆ›å»ºå®Œæˆ: {output_video}")

def main():
    parser = argparse.ArgumentParser(description="YOLO-SAM2 è§†é¢‘åˆ†å‰²æ¼”ç¤º")
    parser.add_argument("--video", required=True, help="è¾“å…¥è§†é¢‘è·¯å¾„")
    parser.add_argument("--yolo", default="runs/detect/lajiao_detection_20250623_053550/weights/best.pt", help="YOLOæ¨¡å‹è·¯å¾„")
    parser.add_argument("--sam2", default="/home/zcx/sam2/checkpoints/sam2.1_hiera_base_plus.pt", help="SAM2æ¨¡å‹è·¯å¾„")
    parser.add_argument("--conf", type=float, default=0.25, help="YOLOç½®ä¿¡åº¦é˜ˆå€¼")
    parser.add_argument("--ref-frame", type=int, default=0, help="å‚è€ƒå¸§ç´¢å¼•")
    parser.add_argument("--max-frames", type=int, default=50, help="æœ€å¤§å¤„ç†å¸§æ•°")
    parser.add_argument("--output", default="yolo_sam2_results", help="è¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    print("ğŸ¯ YOLO-SAM2 è§†é¢‘åˆ†å‰²æ¼”ç¤º")
    print("=" * 50)
    print(f"è¾“å…¥è§†é¢‘: {args.video}")
    print(f"YOLOæ¨¡å‹: {args.yolo}")
    print(f"SAM2æ¨¡å‹: {args.sam2}")
    print(f"ç½®ä¿¡åº¦é˜ˆå€¼: {args.conf}")
    print(f"å‚è€ƒå¸§: {args.ref_frame}")
    print(f"æœ€å¤§å¸§æ•°: {args.max_frames}")
    print(f"è¾“å‡ºç›®å½•: {args.output}")
    print()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.video):
        print(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {args.video}")
        return
    
    if not os.path.exists(args.yolo):
        print(f"âŒ YOLOæ¨¡å‹ä¸å­˜åœ¨: {args.yolo}")
        return
    
    if not os.path.exists(args.sam2):
        print(f"âŒ SAM2æ¨¡å‹ä¸å­˜åœ¨: {args.sam2}")
        return
    
    try:
        # 1. åŠ è½½æ¨¡å‹
        yolo_model, sam2_model = load_models(args.yolo, args.sam2)
        
        # 2. æå–å¸§
        frame_dir = "temp_frames"
        frame_count = extract_frames(args.video, frame_dir, args.max_frames)
        
        if frame_count == 0:
            print("âŒ æœªèƒ½æå–åˆ°è§†é¢‘å¸§")
            return
        
        # 3. YOLOæ£€æµ‹
        ref_frame_path = os.path.join(frame_dir, f"{args.ref_frame:04d}.jpg")
        if not os.path.exists(ref_frame_path):
            print(f"âŒ å‚è€ƒå¸§ä¸å­˜åœ¨: {ref_frame_path}")
            return
        
        print(f"ğŸ¯ åœ¨å‚è€ƒå¸§ {args.ref_frame} è¿›è¡ŒYOLOæ£€æµ‹...")
        detections = yolo_detect(yolo_model, ref_frame_path, args.conf)
        print(f"âœ… æ£€æµ‹åˆ° {len(detections)} ä¸ªç›®æ ‡")
        
        if len(detections) == 0:
            print("âŒ æœªæ£€æµ‹åˆ°ä»»ä½•ç›®æ ‡ï¼Œè¯·è°ƒæ•´ç½®ä¿¡åº¦é˜ˆå€¼æˆ–é€‰æ‹©å…¶ä»–å‚è€ƒå¸§")
            return
        
        # æ˜¾ç¤ºæ£€æµ‹ç»“æœ
        for i, det in enumerate(detections):
            print(f"   ç›®æ ‡ {i+1}: ç½®ä¿¡åº¦ {det['confidence']:.3f}, ä¸­å¿ƒç‚¹ {det['center']}")
        
        # 4. SAM2åˆ†å‰²
        video_segments = sam2_segment(sam2_model, frame_dir, detections, args.ref_frame)
        
        if len(video_segments) == 0:
            print("âŒ SAM2åˆ†å‰²å¤±è´¥")
            return
        
        # 5. ä¿å­˜ç»“æœ
        save_results(frame_dir, video_segments, detections, args.output)
        
        # 6. åˆ›å»ºè§†é¢‘
        output_video = os.path.join(args.output, "segmented_video.mp4")
        create_video(args.output, output_video)
        
        print("\nğŸ‰ å¤„ç†å®Œæˆï¼")
        print(f"ğŸ“ ç»“æœç›®å½•: {args.output}")
        print(f"ğŸ¬ åˆ†å‰²è§†é¢‘: {output_video}")
        print(f"ğŸ–¼ï¸ æ©ç æ–‡ä»¶: {args.output}/masks/")
        print(f"ğŸ‘ï¸ å¯è§†åŒ–å›¾åƒ: {args.output}/visualizations/")
        
    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists("temp_frames"):
            import shutil
            shutil.rmtree("temp_frames")
            print("ğŸ§¹ ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†")

if __name__ == "__main__":
    main() 