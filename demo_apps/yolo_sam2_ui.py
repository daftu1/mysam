#!/usr/bin/env python3
"""
YOLO-SAM2 è§†é¢‘åˆ†å‰²UI
ç»“åˆYOLO11ç›®æ ‡æ£€æµ‹å’ŒSAM2ç²¾ç¡®åˆ†å‰²çš„å®Œæ•´è§£å†³æ–¹æ¡ˆ
"""

import streamlit as st
import cv2
import numpy as np
import torch
import os
import tempfile
import shutil
from pathlib import Path
from PIL import Image
import uuid
from ultralytics import YOLO, SAM

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="YOLO-SAM2 è§†é¢‘åˆ†å‰²ç³»ç»Ÿ",
    page_icon="ğŸ¯",
    layout="wide",
    initial_sidebar_state="expanded"
)

@st.cache_resource
def load_yolo_model(model_path):
    """åŠ è½½YOLO11æ¨¡å‹"""
    try:
        model = YOLO(model_path)
        return model
    except Exception as e:
        st.error(f"YOLOæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None

@st.cache_resource
def load_sam2_model(model_name="sam2.1_b.pt"):
    """åŠ è½½SAM2æ¨¡å‹ - ä½¿ç”¨ultralyticsçš„SAM"""
    try:
        sam2_model = SAM(model_name)
        return sam2_model
    except Exception as e:
        st.error(f"SAM2æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None

def yolo_detect(model, image_path, conf_threshold=0.25):
    """ä½¿ç”¨YOLOæ£€æµ‹ç›®æ ‡"""
    results = model.predict(image_path, conf=conf_threshold, verbose=False)
    
    detections = []
    if len(results) > 0 and len(results[0].boxes) > 0:
        boxes = results[0].boxes
        for i, box in enumerate(boxes):
            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
            conf = box.conf[0].cpu().numpy()
            cls = int(box.cls[0].cpu().numpy())
            
            detections.append({
                'bbox': [int(x1), int(y1), int(x2), int(y2)],
                'confidence': float(conf),
                'class': cls,
                'center': [int((x1+x2)/2), int((y1+y2)/2)]
            })
    
    return detections

def sam2_segment_with_prompts(sam2_model, image_path, detections):
    """ä½¿ç”¨SAM2å¯¹å•å¼ å›¾åƒè¿›è¡Œåˆ†å‰²ï¼ˆåŸºäºæ£€æµ‹ç»“æœæä¾›æç¤ºï¼‰"""
    try:
        if not detections:
            return {}
        
        # å‡†å¤‡æç¤ºç‚¹ï¼ˆä½¿ç”¨æ£€æµ‹æ¡†çš„ä¸­å¿ƒç‚¹ï¼‰
        points = []
        labels = []
        
        for detection in detections:
            center_x, center_y = detection['center']
            points.append([center_x, center_y])
            labels.append(1)  # æ­£ç‚¹
        
        # ä½¿ç”¨ç‚¹æç¤ºè¿›è¡Œåˆ†å‰²
        results = sam2_model.predict(
            image_path, 
            points=points, 
            labels=labels,
            verbose=False
        )
        
        # æå–æ©ç 
        masks = {}
        if len(results) > 0 and hasattr(results[0], 'masks') and results[0].masks is not None:
            for i, mask in enumerate(results[0].masks.data):
                mask_np = mask.cpu().numpy().astype(bool)
                masks[i+1] = mask_np
        
        return masks
        
    except Exception as e:
        st.error(f"SAM2åˆ†å‰²å¤±è´¥: {e}")
        return {}

# æ³¨æ„ï¼šç§»é™¤äº†sam2_segment_videoå‡½æ•°ï¼Œå› ä¸ºultralyticsçš„SAM2
# ä¸æ”¯æŒçœŸæ­£çš„è§†é¢‘æ—¶é—´ä¸€è‡´æ€§åˆ†å‰²ï¼Œåªèƒ½é€å¸§ç‹¬ç«‹å¤„ç†

def process_frame_with_yolo_sam2(yolo_model, sam2_model, frame_path, conf_threshold=0.25):
    """ç»“åˆYOLOæ£€æµ‹å’ŒSAM2åˆ†å‰²å¤„ç†å•å¸§"""
    # YOLOæ£€æµ‹
    detections = yolo_detect(yolo_model, frame_path, conf_threshold)
    
    # SAM2åˆ†å‰²
    masks = {}
    if detections:
        masks = sam2_segment_with_prompts(sam2_model, frame_path, detections)
    
    return detections, masks

def visualize_results(image_path, detections, masks=None, show_boxes=True, show_masks=True):
    """å¯è§†åŒ–æ£€æµ‹å’Œåˆ†å‰²ç»“æœ"""
    image = cv2.imread(image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    overlay = image.copy()
    
    # ç»˜åˆ¶åˆ†å‰²æ©ç 
    if show_masks and masks:
        colors = [
            (255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0),
            (255, 0, 255), (0, 255, 255), (128, 0, 0), (0, 128, 0)
        ]
        
        for obj_id, mask in masks.items():
            if mask.sum() > 0:
                color = colors[(obj_id-1) % len(colors)]
                overlay[mask > 0] = (overlay[mask > 0] * 0.6 + np.array(color) * 0.4).astype(np.uint8)
    
    # ç»˜åˆ¶æ£€æµ‹æ¡†
    if show_boxes and detections:
        for i, detection in enumerate(detections):
            x1, y1, x2, y2 = detection['bbox']
            conf = detection['confidence']
            
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            cv2.rectangle(overlay, (x1, y1), (x2, y2), (0, 255, 0), 2)
            
            # ç»˜åˆ¶ç½®ä¿¡åº¦
            label = f"Object {i+1}: {conf:.2f}"
            cv2.putText(overlay, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
            
            # ç»˜åˆ¶ä¸­å¿ƒç‚¹
            center_x, center_y = detection['center']
            cv2.circle(overlay, (center_x, center_y), 5, (255, 0, 0), -1)
    
    return overlay

def extract_frames(video_path, output_dir, max_frames=None):
    """æå–è§†é¢‘å¸§"""
    os.makedirs(output_dir, exist_ok=True)
    
    cap = cv2.VideoCapture(video_path)
    frame_count = 0
    extracted_frames = []
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        # å¦‚æœè®¾ç½®äº†æœ€å¤§å¸§æ•°é™åˆ¶ï¼Œåˆ™æ£€æŸ¥
        if max_frames is not None and frame_count >= max_frames:
            break
        
        frame_path = os.path.join(output_dir, f"{frame_count:04d}.jpg")
        cv2.imwrite(frame_path, frame)
        extracted_frames.append(frame_path)
        frame_count += 1
    
    cap.release()
    return extracted_frames, frame_count

def main():
    st.title("ğŸ¯ YOLO-SAM2 è§†é¢‘åˆ†å‰²ç³»ç»Ÿ")
    st.markdown("### ç»“åˆYOLO11æ£€æµ‹å’ŒSAM2åˆ†å‰²çš„æ™ºèƒ½è§†é¢‘å¤„ç†ç³»ç»Ÿ")
    
    # ä¾§è¾¹æ é…ç½®
    with st.sidebar:
        st.header("âš™ï¸ æ¨¡å‹é…ç½®")
        
        # YOLOæ¨¡å‹é€‰æ‹©
        st.subheader("ğŸ¯ YOLO11 æ£€æµ‹æ¨¡å‹")
        
        # ç›´æ¥ä½¿ç”¨è®­ç»ƒå¥½çš„è¾£æ¤’æ£€æµ‹æ¨¡å‹
        trained_model_path = "/home/zcx/datasam2get/runs/detect/lajiao_detection_20250623_053550/weights/best.pt"
        yolo_model_path = trained_model_path  # ä¸å†éœ€è¦ç”¨æˆ·é€‰æ‹©
        if os.path.exists(trained_model_path):
            st.success(f"âœ… ä½¿ç”¨è¾£æ¤’ä¸“ç”¨æ£€æµ‹æ¨¡å‹")
            st.info(f"ğŸŒ¶ï¸ æ¨¡å‹è·¯å¾„: {yolo_model_path}")
        else:
            st.error("âŒ è¾£æ¤’æ£€æµ‹æ¨¡å‹æœªæ‰¾åˆ°")
            yolo_model_path = "yolo11n.pt"
            st.warning("âš ï¸ å›é€€åˆ°é¢„è®­ç»ƒæ¨¡å‹: yolo11n.pt")
        
        # æ£€æµ‹å‚æ•°
        conf_threshold = st.slider("ç½®ä¿¡åº¦é˜ˆå€¼", 0.1, 1.0, 0.25, 0.05)
        
        st.divider()
        
        # SAM2æ¨¡å‹é€‰æ‹©
        st.subheader("ğŸ¨ SAM2 åˆ†å‰²æ¨¡å‹")
        
        # é»˜è®¤ä½¿ç”¨SAM2.1-B (å¹³è¡¡å‹)
        sam2_model_name = "sam2.1_b.pt"
        st.success(f"âœ… ä½¿ç”¨SAM2.1-Bæ¨¡å‹ (å¹³è¡¡å‹)")
        
        # é«˜çº§é€‰é¡¹ - å¯é€‰æ‹©å…¶ä»–æ¨¡å‹
        with st.expander("ğŸ”§ é«˜çº§é€‰é¡¹ - æ›´æ¢SAM2æ¨¡å‹"):
            sam2_model_options = {
                "SAM2.1-B (å¹³è¡¡å‹ï¼Œæ¨è)": "sam2.1_b.pt",
                "SAM2.1-L (å¤§å‹ï¼Œé«˜ç²¾åº¦)": "sam2.1_l.pt", 
                "SAM2.1-S (å°å‹ï¼Œå¿«é€Ÿ)": "sam2.1_s.pt",
                "SAM2.1-T (æœ€å°ï¼Œæœ€å¿«)": "sam2.1_t.pt"
            }
            
            selected_sam2 = st.selectbox(
                "é€‰æ‹©SAM2æ¨¡å‹", 
                options=list(sam2_model_options.keys()),
                index=0
            )
            sam2_model_name = sam2_model_options[selected_sam2]
            st.info(f"åˆ‡æ¢ä¸º: {selected_sam2}")
        
        # å¤„ç†æ¨¡å¼
        st.subheader("ğŸ”„ å¤„ç†æ¨¡å¼")
        
        # è¯´æ˜ultralytics SAM2çš„é™åˆ¶
        st.info("â„¹ï¸ æ³¨æ„ï¼šultralyticsçš„SAM2ä¸æ”¯æŒçœŸæ­£çš„è§†é¢‘æ—¶é—´ä¸€è‡´æ€§åˆ†å‰²ï¼Œåªèƒ½é€å¸§ç‹¬ç«‹å¤„ç†")
        
        # åªæä¾›é€å¸§å¤„ç†æ¨¡å¼ï¼Œå› ä¸ºç›´æ¥è§†é¢‘åˆ†å‰²å®é™…ä¸Šä¹Ÿæ˜¯é€å¸§çš„
        st.write("**é€å¸§å¤„ç† (YOLO+SAM2) ğŸŒ¶ï¸** - æ¨èæ¨¡å¼")
        st.write("- ä½¿ç”¨ä½ è®­ç»ƒçš„è¾£æ¤’æ£€æµ‹æ¨¡å‹")
        st.write("- å¯¹æ¯ä¸€å¸§è¿›è¡ŒYOLOæ£€æµ‹ + SAM2åˆ†å‰²")
        st.write("- è™½ç„¶ä¸æ˜¯çœŸæ­£çš„è§†é¢‘åˆ†å‰²ï¼Œä½†ç»“æœæ›´ç²¾ç¡®")
        
        processing_mode = "é€å¸§å¤„ç† (YOLO+SAM2) ğŸŒ¶ï¸"
        
        st.divider()
        
        # æ˜¾ç¤ºé€‰é¡¹
        st.subheader("ğŸ–¼ï¸ æ˜¾ç¤ºé€‰é¡¹")
        show_boxes = st.checkbox("æ˜¾ç¤ºæ£€æµ‹æ¡†", value=True)
        show_masks = st.checkbox("æ˜¾ç¤ºåˆ†å‰²æ©ç ", value=True)
        show_centers = st.checkbox("æ˜¾ç¤ºä¸­å¿ƒç‚¹", value=True)
        
        st.divider()
        
        # ç³»ç»Ÿä¿¡æ¯
        st.subheader("ğŸ’» ç³»ç»Ÿä¿¡æ¯")
        st.write(f"è®¾å¤‡: {'GPU' if torch.cuda.is_available() else 'CPU'}")
        if torch.cuda.is_available():
            st.write(f"GPU: {torch.cuda.get_device_name(0)}")
    
    # ä¸»ç•Œé¢
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.subheader("ğŸ“¹ è§†é¢‘ä¸Šä¼ ")
        uploaded_video = st.file_uploader(
            "é€‰æ‹©è§†é¢‘æ–‡ä»¶", 
            type=['mp4', 'avi', 'mov', 'mkv'],
            help="æ”¯æŒå¸¸è§è§†é¢‘æ ¼å¼"
        )
        
        if uploaded_video:
            # ä¿å­˜ä¸Šä¼ çš„è§†é¢‘
            temp_video_path = f"temp_video_{uuid.uuid4().hex[:8]}.mp4"
            with open(temp_video_path, "wb") as f:
                f.write(uploaded_video.read())
            
            st.video(temp_video_path)
            
            # è§†é¢‘ä¿¡æ¯
            cap = cv2.VideoCapture(temp_video_path)
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = int(cap.get(cv2.CAP_PROP_FPS)) or 30
            duration = frame_count / fps
            cap.release()
            
            st.write(f"ğŸ“Š è§†é¢‘ä¿¡æ¯:")
            st.write(f"- æ€»å¸§æ•°: {frame_count}")
            st.write(f"- å¸§ç‡: {fps} FPS")
            st.write(f"- æ—¶é•¿: {duration:.1f} ç§’")
            
            # å¤„ç†æŒ‰é’®
            if st.button("ğŸš€ å¼€å§‹YOLO-SAM2è§†é¢‘åˆ†å‰²", type="primary"):
                with st.spinner("æ­£åœ¨åŠ è½½æ¨¡å‹..."):
                    # åŠ è½½YOLOå’ŒSAM2æ¨¡å‹
                    yolo_model = load_yolo_model(yolo_model_path)
                    if yolo_model is None:
                        st.error("YOLOæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¨¡å‹è·¯å¾„")
                        return
                    
                    sam2_model = load_sam2_model(sam2_model_name)
                    if sam2_model is None:
                        st.error("SAM2æ¨¡å‹åŠ è½½å¤±è´¥")
                        return
                    
                    st.success(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
                
                # é€å¸§å¤„ç†æ¨¡å¼
                    with st.spinner("æ­£åœ¨æå–è§†é¢‘å¸§..."):
                        # æå–å¸§
                        frame_dir = f"frames_{uuid.uuid4().hex[:8]}"
                        
                        # å…ˆè·å–è§†é¢‘æ€»å¸§æ•°ç”¨äºè¿›åº¦æ˜¾ç¤º
                        cap_info = cv2.VideoCapture(temp_video_path)
                        total_video_frames = int(cap_info.get(cv2.CAP_PROP_FRAME_COUNT))
                        cap_info.release()
                        
                        st.info(f"ğŸ¬ è§†é¢‘æ€»å¸§æ•°: {total_video_frames}ï¼Œå¼€å§‹æå–æ‰€æœ‰å¸§...")
                        
                        frame_paths, total_frames = extract_frames(temp_video_path, frame_dir)
                        st.success(f"âœ… æå–äº† {total_frames} å¸§")
                    
                    # é€‰æ‹©å‚è€ƒå¸§è¿›è¡Œæ£€æµ‹
                    reference_frame = st.slider(
                        "é€‰æ‹©å‚è€ƒå¸§è¿›è¡Œç›®æ ‡æ£€æµ‹", 
                        0, total_frames-1, 
                        min(10, total_frames//2)
                    )
                    
                    if frame_paths:
                        # åœ¨å‚è€ƒå¸§ä¸Šè¿›è¡ŒYOLOæ£€æµ‹
                        with st.spinner(f"æ­£åœ¨æ£€æµ‹ç¬¬ {reference_frame} å¸§çš„ç›®æ ‡..."):
                            detections = yolo_detect(yolo_model, frame_paths[reference_frame], conf_threshold)
                            st.success(f"âœ… æ£€æµ‹åˆ° {len(detections)} ä¸ªç›®æ ‡")
                        
                        if detections:
                            # æ˜¾ç¤ºæ£€æµ‹ç»“æœ
                            st.subheader(f"ğŸ¯ ç¬¬ {reference_frame} å¸§æ£€æµ‹ç»“æœ")
                            
                            # å¯è§†åŒ–æ£€æµ‹ç»“æœ
                            detection_image = visualize_results(
                                frame_paths[reference_frame], 
                                detections, 
                                show_boxes=show_boxes
                            )
                            st.image(detection_image, caption=f"æ£€æµ‹ç»“æœ - ç¬¬ {reference_frame} å¸§")
                            
                            # æ˜¾ç¤ºæ£€æµ‹è¯¦æƒ…
                            for i, det in enumerate(detections):
                                st.write(f"ç›®æ ‡ {i+1}: ç½®ä¿¡åº¦ {det['confidence']:.3f}, ä¸­å¿ƒç‚¹ {det['center']}")
                            
                            # å¼€å§‹é€å¸§å¤„ç†
                            if st.button("ğŸ¨ å¼€å§‹é€å¸§YOLO-SAM2å¤„ç†"):
                                with st.spinner("æ­£åœ¨è¿›è¡Œé€å¸§YOLO-SAM2å¤„ç†..."):
                                    video_segments = {}
                                    all_detections = {}
                                    
                                    # æ˜¾ç¤ºå¤„ç†ä¿¡æ¯
                                    st.info(f"ğŸ“Š å¼€å§‹å¤„ç† {len(frame_paths)} å¸§ï¼Œè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´...")
                                    
                                    progress_bar = st.progress(0)
                                    status_text = st.empty()
                                    
                                    for i, frame_path in enumerate(frame_paths):
                                        # æ›´æ–°çŠ¶æ€
                                        status_text.text(f"æ­£åœ¨å¤„ç†ç¬¬ {i+1}/{len(frame_paths)} å¸§...")
                                        
                                        # å¤„ç†æ¯ä¸€å¸§
                                        frame_detections, frame_masks = process_frame_with_yolo_sam2(
                                            yolo_model, sam2_model, frame_path, conf_threshold
                                        )
                                        
                                        if frame_masks:
                                            video_segments[i] = frame_masks
                                        if frame_detections:
                                            all_detections[i] = frame_detections
                                        
                                        # æ›´æ–°è¿›åº¦æ¡
                                        progress_bar.progress((i + 1) / len(frame_paths))
                                    
                                    # æ¸…é™¤çŠ¶æ€æ–‡æœ¬
                                    status_text.empty()
                                    
                                    if video_segments:
                                        st.success(f"âœ… é€å¸§å¤„ç†å®Œæˆï¼æˆåŠŸå¤„ç†äº† {len(video_segments)} å¸§")
                                        st.info(f"ğŸ“ˆ æ£€æµ‹ç»Ÿè®¡: å…±åœ¨ {len(all_detections)} å¸§ä¸­å‘ç°ç›®æ ‡")
                                        
                                        # ä¿å­˜ç»“æœåˆ°session state
                                        st.session_state['video_segments'] = video_segments
                                        st.session_state['frame_paths'] = frame_paths
                                        st.session_state['all_detections'] = all_detections
                                        st.session_state['frame_dir'] = frame_dir
                        else:
                            st.warning("âš ï¸ æœªæ£€æµ‹åˆ°ä»»ä½•ç›®æ ‡ï¼Œè¯·è°ƒæ•´ç½®ä¿¡åº¦é˜ˆå€¼æˆ–é€‰æ‹©å…¶ä»–å¸§")
    
    with col2:
        st.subheader("ğŸ¨ åˆ†å‰²ç»“æœé¢„è§ˆ")
        
        # å¦‚æœæœ‰åˆ†å‰²ç»“æœï¼Œæ˜¾ç¤ºé¢„è§ˆ
        if 'video_segments' in st.session_state:
            video_segments = st.session_state['video_segments']
            frame_paths = st.session_state['frame_paths']
            all_detections = st.session_state.get('all_detections', {})
            
            # å¸§é€‰æ‹©å™¨
            max_frame = len(frame_paths)-1 if frame_paths else 0
            if video_segments:
                max_frame = max(max_frame, max(video_segments.keys()))
            preview_frame = st.slider("é¢„è§ˆå¸§", 0, max_frame, 0)
            
            if preview_frame in video_segments:
                masks = video_segments[preview_frame]
                detections = all_detections.get(preview_frame, [])
                
                # å¯è§†åŒ–ç»“æœ
                result_image = visualize_results(
                    frame_paths[preview_frame],
                    detections,
                    masks,
                    show_boxes=show_boxes,
                    show_masks=show_masks
                )
                
                st.image(result_image, caption=f"YOLO+SAM2åˆ†å‰²ç»“æœ - ç¬¬ {preview_frame} å¸§")
                
                # æ˜¾ç¤ºæ©ç ç»Ÿè®¡
                st.write("ğŸ“Š åˆ†å‰²ç»Ÿè®¡:")
                for obj_id, mask in masks.items():
                    pixel_count = mask.sum()
                    st.write(f"- è¾£æ¤’ç›®æ ‡ {obj_id}: {pixel_count} åƒç´ ")
                
                # æ˜¾ç¤ºæ£€æµ‹ä¿¡æ¯
                if detections:
                    st.write("ğŸ¯ æ£€æµ‹ä¿¡æ¯:")
                    for i, det in enumerate(detections):
                        st.write(f"- æ£€æµ‹ {i+1}: ç½®ä¿¡åº¦ {det['confidence']:.3f}")
            else:
                st.info("è¯¥å¸§æ— åˆ†å‰²ç»“æœ")
            
            # å¯¼å‡ºé€‰é¡¹
            st.subheader("ğŸ“¤ å¯¼å‡ºé€‰é¡¹")
            
            col_a, col_b = st.columns(2)
            
            with col_a:
                if st.button("ğŸ’¾ ä¿å­˜åˆ†å‰²æ©ç "):
                    output_dir = f"yolo_sam2_output_{uuid.uuid4().hex[:8]}"
                    os.makedirs(output_dir, exist_ok=True)
                    
                    saved_count = 0
                    for frame_idx, masks in video_segments.items():
                        if masks:
                            for obj_id, mask in masks.items():
                                if mask.sum() > 0:
                                    mask_path = os.path.join(output_dir, f"frame_{frame_idx:04d}_obj_{obj_id}.png")
                                    mask_image = (mask * 255).astype(np.uint8)
                                    cv2.imwrite(mask_path, mask_image)
                                    saved_count += 1
                    
                    st.success(f"âœ… ä¿å­˜äº† {saved_count} ä¸ªæ©ç åˆ° {output_dir}")
            
            with col_b:
                if st.button("ğŸ¬ ç”Ÿæˆåˆ†å‰²è§†é¢‘"):
                    st.info("ğŸ”„ è§†é¢‘ç”ŸæˆåŠŸèƒ½å¼€å‘ä¸­...")
                    
        else:
            st.info("ğŸ‘† è¯·å…ˆä¸Šä¼ è§†é¢‘å¹¶å®Œæˆæ£€æµ‹åˆ†å‰²")
            
            # æ˜¾ç¤ºç¤ºä¾‹
            st.markdown(f"""
            ### ğŸ”„ å¤„ç†æµç¨‹
            
            **YOLO+SAM2é€å¸§å¤„ç† ğŸŒ¶ï¸**
            1. ä¸Šä¼ è§†é¢‘å¹¶æå–æ‰€æœ‰å¸§
            2. ä½¿ç”¨ä½ çš„è¾£æ¤’æ£€æµ‹æ¨¡å‹æ‰¾åˆ°ç›®æ ‡
            3. SAM2åŸºäºæ£€æµ‹ç»“æœè¿›è¡Œç²¾ç¡®åˆ†å‰²
            4. æŸ¥çœ‹ç»“æœå¹¶å¯¼å‡º
            
            ### âš ï¸ é‡è¦è¯´æ˜
            - **ultralyticsçš„SAM2ä¸æ”¯æŒçœŸæ­£çš„è§†é¢‘æ—¶é—´ä¸€è‡´æ€§åˆ†å‰²**
            - åªèƒ½å¯¹æ¯ä¸€å¸§ç‹¬ç«‹è¿›è¡Œåˆ†å‰²å¤„ç†
            - å¦‚éœ€çœŸæ­£çš„è§†é¢‘åˆ†å‰²ï¼Œéœ€è¦ä½¿ç”¨MetaåŸç‰ˆSAM2
            
            ### âœ¨ åŠŸèƒ½ç‰¹æ€§
            - ğŸŒ¶ï¸ **ä¸“ç”¨æ£€æµ‹**: ä½¿ç”¨ä½ è®­ç»ƒçš„è¾£æ¤’æ£€æµ‹æ¨¡å‹
            - ğŸ¯ **ç²¾ç¡®åˆ†å‰²**: YOLOæ£€æµ‹+SAM2åƒç´ çº§åˆ†å‰²
            - ğŸ¬ **å®Œæ•´å¤„ç†**: å¤„ç†è§†é¢‘æ‰€æœ‰å¸§ï¼Œä¸é™åˆ¶æ•°é‡
            - ğŸ“Š **å®æ—¶è¿›åº¦**: æ˜¾ç¤ºå¤„ç†è¿›åº¦å’Œç»Ÿè®¡ä¿¡æ¯
            - ğŸ’¾ **ç»“æœå¯¼å‡º**: æ”¯æŒæ©ç å’Œè§†é¢‘å¯¼å‡º
            """)

    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    if st.sidebar.button("ğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶"):
        # æ¸…ç†ä¸´æ—¶è§†é¢‘å’Œå¸§ç›®å½•
        for item in os.listdir('.'):
            if item.startswith('temp_video_') or item.startswith('frames_'):
                if os.path.isfile(item):
                    os.remove(item)
                elif os.path.isdir(item):
                    shutil.rmtree(item)
        
        # æ¸…ç†session state
        keys_to_clear = ['video_segments', 'frame_paths', 'all_detections', 'frame_dir']
        for key in keys_to_clear:
            if key in st.session_state:
                del st.session_state[key]
        
        st.success("âœ… ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†")
        st.rerun()

if __name__ == "__main__":
    main() 