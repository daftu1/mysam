#!/usr/bin/env python3
"""
YOLO-SAM2 è§†é¢‘åˆ†å‰²UI
ç»“åˆYOLO11ç›®æ ‡æ£€æµ‹å’ŒSAM2ç²¾ç¡®åˆ†å‰²çš„å®Œæ•´è§£å†³æ–¹æ¡ˆ
"""

import streamlit as st
import cv2
import numpy as np
import torch
import os
import tempfile
import shutil
from pathlib import Path
from PIL import Image
import uuid
from ultralytics import YOLO
import sys

# æ·»åŠ SAM2è·¯å¾„
sys.path.append('/home/zcx/sam2')
from sam2.build_sam import build_sam2_video_predictor

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="YOLO-SAM2 è§†é¢‘åˆ†å‰²ç³»ç»Ÿ",
    page_icon="ğŸ¯",
    layout="wide",
    initial_sidebar_state="expanded"
)

@st.cache_resource
def load_yolo_model(model_path):
    """åŠ è½½YOLO11æ¨¡å‹"""
    try:
        model = YOLO(model_path)
        return model
    except Exception as e:
        st.error(f"YOLOæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None

@st.cache_resource
def load_sam2_model():
    """åŠ è½½SAM2æ¨¡å‹"""
    try:
        checkpoint = "/home/zcx/sam2/checkpoints/sam2.1_hiera_base_plus.pt"
        model_cfg = "configs/sam2.1/sam2.1_hiera_b+.yaml"
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        sam2_model = build_sam2_video_predictor(model_cfg, checkpoint, device=device)
        return sam2_model
    except Exception as e:
        st.error(f"SAM2æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None

def extract_frames(video_path, output_dir, max_frames=100):
    """æå–è§†é¢‘å¸§"""
    os.makedirs(output_dir, exist_ok=True)
    
    cap = cv2.VideoCapture(video_path)
    frame_count = 0
    extracted_frames = []
    
    while frame_count < max_frames:
        ret, frame = cap.read()
        if not ret:
            break
        
        frame_path = os.path.join(output_dir, f"{frame_count:04d}.jpg")
        cv2.imwrite(frame_path, frame)
        extracted_frames.append(frame_path)
        frame_count += 1
    
    cap.release()
    return extracted_frames, frame_count

def yolo_detect(model, image_path, conf_threshold=0.25):
    """ä½¿ç”¨YOLOæ£€æµ‹ç›®æ ‡"""
    results = model.predict(image_path, conf=conf_threshold, verbose=False)
    
    detections = []
    if len(results) > 0 and len(results[0].boxes) > 0:
        boxes = results[0].boxes
        for i, box in enumerate(boxes):
            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
            conf = box.conf[0].cpu().numpy()
            cls = int(box.cls[0].cpu().numpy())
            
            detections.append({
                'bbox': [int(x1), int(y1), int(x2), int(y2)],
                'confidence': float(conf),
                'class': cls,
                'center': [int((x1+x2)/2), int((y1+y2)/2)]
            })
    
    return detections

def sam2_segment_video(sam2_model, frame_dir, detections, target_frame=0):
    """ä½¿ç”¨SAM2è¿›è¡Œè§†é¢‘åˆ†å‰²"""
    try:
        # åˆå§‹åŒ–SAM2æ¨ç†çŠ¶æ€
        inference_state = sam2_model.init_state(video_path=frame_dir)
        
        video_segments = {}
        
        # ä¸ºæ¯ä¸ªæ£€æµ‹åˆ°çš„ç›®æ ‡åˆ›å»ºåˆ†å‰²
        for obj_id, detection in enumerate(detections, 1):
            # ä½¿ç”¨æ£€æµ‹æ¡†çš„ä¸­å¿ƒç‚¹ä½œä¸ºæ­£ç‚¹
            center_x, center_y = detection['center']
            points = np.array([[center_x, center_y]], dtype=np.float32)
            labels = np.array([1], dtype=np.int32)  # æ­£ç‚¹
            
            # åœ¨ç›®æ ‡å¸§æ·»åŠ ç‚¹
            _, _, _ = sam2_model.add_new_points_or_box(
                inference_state=inference_state,
                frame_idx=target_frame,
                obj_id=obj_id,
                points=points,
                labels=labels,
            )
        
        # ä¼ æ’­åˆ°æ•´ä¸ªè§†é¢‘
        for out_frame_idx, out_obj_ids, out_mask_logits in sam2_model.propagate_in_video(inference_state):
            video_segments[out_frame_idx] = {}
            for i, obj_id in enumerate(out_obj_ids):
                mask = (out_mask_logits[i] > 0.0).cpu().numpy().squeeze()
                video_segments[out_frame_idx][obj_id] = mask
        
        return video_segments, inference_state
        
    except Exception as e:
        st.error(f"SAM2åˆ†å‰²å¤±è´¥: {e}")
        return {}, None

def visualize_results(image_path, detections, masks=None, show_boxes=True, show_masks=True):
    """å¯è§†åŒ–æ£€æµ‹å’Œåˆ†å‰²ç»“æœ"""
    image = cv2.imread(image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    overlay = image.copy()
    
    # ç»˜åˆ¶åˆ†å‰²æ©ç 
    if show_masks and masks:
        colors = [
            (255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0),
            (255, 0, 255), (0, 255, 255), (128, 0, 0), (0, 128, 0)
        ]
        
        for obj_id, mask in masks.items():
            if mask.sum() > 0:
                color = colors[(obj_id-1) % len(colors)]
                overlay[mask > 0] = (overlay[mask > 0] * 0.6 + np.array(color) * 0.4).astype(np.uint8)
    
    # ç»˜åˆ¶æ£€æµ‹æ¡†
    if show_boxes and detections:
        for i, detection in enumerate(detections):
            x1, y1, x2, y2 = detection['bbox']
            conf = detection['confidence']
            
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            cv2.rectangle(overlay, (x1, y1), (x2, y2), (0, 255, 0), 2)
            
            # ç»˜åˆ¶ç½®ä¿¡åº¦
            label = f"Object {i+1}: {conf:.2f}"
            cv2.putText(overlay, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
            
            # ç»˜åˆ¶ä¸­å¿ƒç‚¹
            center_x, center_y = detection['center']
            cv2.circle(overlay, (center_x, center_y), 5, (255, 0, 0), -1)
    
    return overlay

def main():
    st.title("ğŸ¯ YOLO-SAM2 è§†é¢‘åˆ†å‰²ç³»ç»Ÿ")
    st.markdown("### ç»“åˆYOLO11æ£€æµ‹å’ŒSAM2åˆ†å‰²çš„æ™ºèƒ½è§†é¢‘å¤„ç†ç³»ç»Ÿ")
    
    # ä¾§è¾¹æ é…ç½®
    with st.sidebar:
        st.header("âš™ï¸ æ¨¡å‹é…ç½®")
        
        # YOLOæ¨¡å‹é€‰æ‹©
        st.subheader("ğŸ¯ YOLO11 æ£€æµ‹æ¨¡å‹")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹
        trained_model_path = "runs/detect/lajiao_detection_20250623_053550/weights/best.pt"
        if os.path.exists(trained_model_path):
            use_trained_model = st.checkbox("ä½¿ç”¨è®­ç»ƒå¥½çš„è¾£æ¤’æ£€æµ‹æ¨¡å‹", value=True)
            if use_trained_model:
                yolo_model_path = trained_model_path
                st.success(f"âœ… ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹: {yolo_model_path}")
            else:
                yolo_model_path = st.text_input("YOLOæ¨¡å‹è·¯å¾„", value="yolo11n.pt")
        else:
            yolo_model_path = st.text_input("YOLOæ¨¡å‹è·¯å¾„", value="yolo11n.pt")
        
        # æ£€æµ‹å‚æ•°
        conf_threshold = st.slider("ç½®ä¿¡åº¦é˜ˆå€¼", 0.1, 1.0, 0.25, 0.05)
        
        st.divider()
        
        # SAM2æ¨¡å‹çŠ¶æ€
        st.subheader("ğŸ¨ SAM2 åˆ†å‰²æ¨¡å‹")
        if os.path.exists("/home/zcx/sam2/checkpoints/sam2.1_hiera_base_plus.pt"):
            st.success("âœ… SAM2æ¨¡å‹å·²å°±ç»ª")
        else:
            st.error("âŒ SAM2æ¨¡å‹æœªæ‰¾åˆ°")
        
        st.divider()
        
        # æ˜¾ç¤ºé€‰é¡¹
        st.subheader("ğŸ–¼ï¸ æ˜¾ç¤ºé€‰é¡¹")
        show_boxes = st.checkbox("æ˜¾ç¤ºæ£€æµ‹æ¡†", value=True)
        show_masks = st.checkbox("æ˜¾ç¤ºåˆ†å‰²æ©ç ", value=True)
        show_centers = st.checkbox("æ˜¾ç¤ºä¸­å¿ƒç‚¹", value=True)
        
        st.divider()
        
        # ç³»ç»Ÿä¿¡æ¯
        st.subheader("ğŸ’» ç³»ç»Ÿä¿¡æ¯")
        st.write(f"è®¾å¤‡: {'GPU' if torch.cuda.is_available() else 'CPU'}")
        if torch.cuda.is_available():
            st.write(f"GPU: {torch.cuda.get_device_name(0)}")
    
    # ä¸»ç•Œé¢
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.subheader("ğŸ“¹ è§†é¢‘ä¸Šä¼ ")
        uploaded_video = st.file_uploader(
            "é€‰æ‹©è§†é¢‘æ–‡ä»¶", 
            type=['mp4', 'avi', 'mov', 'mkv'],
            help="æ”¯æŒå¸¸è§è§†é¢‘æ ¼å¼"
        )
        
        if uploaded_video:
            # ä¿å­˜ä¸Šä¼ çš„è§†é¢‘
            temp_video_path = f"temp_video_{uuid.uuid4().hex[:8]}.mp4"
            with open(temp_video_path, "wb") as f:
                f.write(uploaded_video.read())
            
            st.video(temp_video_path)
            
            # è§†é¢‘ä¿¡æ¯
            cap = cv2.VideoCapture(temp_video_path)
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = int(cap.get(cv2.CAP_PROP_FPS)) or 30
            duration = frame_count / fps
            cap.release()
            
            st.write(f"ğŸ“Š è§†é¢‘ä¿¡æ¯:")
            st.write(f"- æ€»å¸§æ•°: {frame_count}")
            st.write(f"- å¸§ç‡: {fps} FPS")
            st.write(f"- æ—¶é•¿: {duration:.1f} ç§’")
            
            # å¤„ç†æŒ‰é’®
            if st.button("ğŸš€ å¼€å§‹YOLO-SAM2åˆ†å‰²", type="primary"):
                with st.spinner("æ­£åœ¨åŠ è½½æ¨¡å‹..."):
                    # åŠ è½½æ¨¡å‹
                    yolo_model = load_yolo_model(yolo_model_path)
                    sam2_model = load_sam2_model()
                    
                    if yolo_model is None or sam2_model is None:
                        st.error("æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¨¡å‹è·¯å¾„")
                        return
                
                with st.spinner("æ­£åœ¨æå–è§†é¢‘å¸§..."):
                    # æå–å¸§
                    frame_dir = f"frames_{uuid.uuid4().hex[:8]}"
                    frame_paths, total_frames = extract_frames(temp_video_path, frame_dir, max_frames=100)
                    st.success(f"âœ… æå–äº† {total_frames} å¸§")
                
                # é€‰æ‹©å‚è€ƒå¸§è¿›è¡Œæ£€æµ‹
                reference_frame = st.slider(
                    "é€‰æ‹©å‚è€ƒå¸§è¿›è¡Œç›®æ ‡æ£€æµ‹", 
                    0, min(total_frames-1, 99), 
                    min(10, total_frames//2)
                )
                
                if frame_paths:
                    # åœ¨å‚è€ƒå¸§ä¸Šè¿›è¡ŒYOLOæ£€æµ‹
                    with st.spinner(f"æ­£åœ¨æ£€æµ‹ç¬¬ {reference_frame} å¸§çš„ç›®æ ‡..."):
                        detections = yolo_detect(yolo_model, frame_paths[reference_frame], conf_threshold)
                        st.success(f"âœ… æ£€æµ‹åˆ° {len(detections)} ä¸ªç›®æ ‡")
                    
                    if detections:
                        # æ˜¾ç¤ºæ£€æµ‹ç»“æœ
                        st.subheader(f"ğŸ¯ ç¬¬ {reference_frame} å¸§æ£€æµ‹ç»“æœ")
                        
                        # å¯è§†åŒ–æ£€æµ‹ç»“æœ
                        detection_image = visualize_results(
                            frame_paths[reference_frame], 
                            detections, 
                            show_boxes=show_boxes
                        )
                        st.image(detection_image, caption=f"æ£€æµ‹ç»“æœ - ç¬¬ {reference_frame} å¸§")
                        
                        # æ˜¾ç¤ºæ£€æµ‹è¯¦æƒ…
                        for i, det in enumerate(detections):
                            st.write(f"ç›®æ ‡ {i+1}: ç½®ä¿¡åº¦ {det['confidence']:.3f}, ä¸­å¿ƒç‚¹ {det['center']}")
                        
                        # å¼€å§‹SAM2åˆ†å‰²
                        if st.button("ğŸ¨ å¼€å§‹SAM2è§†é¢‘åˆ†å‰²"):
                            with st.spinner("æ­£åœ¨è¿›è¡ŒSAM2è§†é¢‘åˆ†å‰²..."):
                                video_segments, inference_state = sam2_segment_video(
                                    sam2_model, frame_dir, detections, reference_frame
                                )
                                
                                if video_segments:
                                    st.success(f"âœ… åˆ†å‰²å®Œæˆï¼å¤„ç†äº† {len(video_segments)} å¸§")
                                    
                                    # ä¿å­˜ç»“æœåˆ°session state
                                    st.session_state['video_segments'] = video_segments
                                    st.session_state['frame_paths'] = frame_paths
                                    st.session_state['detections'] = detections
                                    st.session_state['frame_dir'] = frame_dir
                    else:
                        st.warning("âš ï¸ æœªæ£€æµ‹åˆ°ä»»ä½•ç›®æ ‡ï¼Œè¯·è°ƒæ•´ç½®ä¿¡åº¦é˜ˆå€¼æˆ–é€‰æ‹©å…¶ä»–å¸§")
    
    with col2:
        st.subheader("ğŸ¨ åˆ†å‰²ç»“æœé¢„è§ˆ")
        
        # å¦‚æœæœ‰åˆ†å‰²ç»“æœï¼Œæ˜¾ç¤ºé¢„è§ˆ
        if 'video_segments' in st.session_state:
            video_segments = st.session_state['video_segments']
            frame_paths = st.session_state['frame_paths']
            detections = st.session_state['detections']
            
            # å¸§é€‰æ‹©å™¨
            max_frame = min(len(frame_paths)-1, max(video_segments.keys()) if video_segments else 0)
            preview_frame = st.slider("é¢„è§ˆå¸§", 0, max_frame, 0)
            
            if preview_frame in video_segments:
                masks = video_segments[preview_frame]
                
                # å¯è§†åŒ–ç»“æœ
                result_image = visualize_results(
                    frame_paths[preview_frame],
                    detections,
                    masks,
                    show_boxes=show_boxes,
                    show_masks=show_masks
                )
                
                st.image(result_image, caption=f"åˆ†å‰²ç»“æœ - ç¬¬ {preview_frame} å¸§")
                
                # æ˜¾ç¤ºæ©ç ç»Ÿè®¡
                st.write("ğŸ“Š åˆ†å‰²ç»Ÿè®¡:")
                for obj_id, mask in masks.items():
                    pixel_count = mask.sum()
                    st.write(f"- ç›®æ ‡ {obj_id}: {pixel_count} åƒç´ ")
            else:
                st.info("è¯¥å¸§æ— åˆ†å‰²ç»“æœ")
            
            # å¯¼å‡ºé€‰é¡¹
            st.subheader("ğŸ“¤ å¯¼å‡ºé€‰é¡¹")
            
            col_a, col_b = st.columns(2)
            
            with col_a:
                if st.button("ğŸ’¾ ä¿å­˜åˆ†å‰²æ©ç "):
                    output_dir = f"yolo_sam2_output_{uuid.uuid4().hex[:8]}"
                    os.makedirs(output_dir, exist_ok=True)
                    
                    saved_count = 0
                    for frame_idx, masks in video_segments.items():
                        if masks:
                            for obj_id, mask in masks.items():
                                if mask.sum() > 0:
                                    mask_path = os.path.join(output_dir, f"frame_{frame_idx:04d}_obj_{obj_id}.png")
                                    mask_image = (mask * 255).astype(np.uint8)
                                    cv2.imwrite(mask_path, mask_image)
                                    saved_count += 1
                    
                    st.success(f"âœ… ä¿å­˜äº† {saved_count} ä¸ªæ©ç åˆ° {output_dir}")
            
            with col_b:
                if st.button("ğŸ¬ ç”Ÿæˆåˆ†å‰²è§†é¢‘"):
                    output_video = f"segmented_video_{uuid.uuid4().hex[:8]}.mp4"
                    
                    # è·å–ç¬¬ä¸€å¸§å°ºå¯¸
                    first_frame = cv2.imread(frame_paths[0])
                    height, width = first_frame.shape[:2]
                    
                    # åˆ›å»ºè§†é¢‘å†™å…¥å™¨
                    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
                    out = cv2.VideoWriter(output_video, fourcc, 10.0, (width, height))
                    
                    for i, frame_path in enumerate(frame_paths):
                        if i in video_segments:
                            # æœ‰åˆ†å‰²ç»“æœçš„å¸§
                            result_frame = visualize_results(
                                frame_path,
                                detections,
                                video_segments[i],
                                show_boxes=show_boxes,
                                show_masks=show_masks
                            )
                            result_frame = cv2.cvtColor(result_frame, cv2.COLOR_RGB2BGR)
                        else:
                            # åŸå§‹å¸§
                            result_frame = cv2.imread(frame_path)
                        
                        out.write(result_frame)
                    
                    out.release()
                    st.success(f"âœ… åˆ†å‰²è§†é¢‘å·²ä¿å­˜: {output_video}")
                    
                    # æ˜¾ç¤ºç”Ÿæˆçš„è§†é¢‘
                    st.video(output_video)
        else:
            st.info("ğŸ‘† è¯·å…ˆä¸Šä¼ è§†é¢‘å¹¶å®Œæˆæ£€æµ‹åˆ†å‰²")
            
            # æ˜¾ç¤ºç¤ºä¾‹
            st.markdown("""
            ### ğŸ”„ å¤„ç†æµç¨‹
            1. **ä¸Šä¼ è§†é¢‘** - æ”¯æŒMP4ã€AVIç­‰æ ¼å¼
            2. **YOLOæ£€æµ‹** - åœ¨å‚è€ƒå¸§æ£€æµ‹ç›®æ ‡
            3. **SAM2åˆ†å‰²** - åŸºäºæ£€æµ‹ç»“æœè¿›è¡Œç²¾ç¡®åˆ†å‰²
            4. **ç»“æœé¢„è§ˆ** - æŸ¥çœ‹åˆ†å‰²æ•ˆæœ
            5. **å¯¼å‡ºç»“æœ** - ä¿å­˜æ©ç æˆ–ç”Ÿæˆè§†é¢‘
            
            ### âœ¨ ç‰¹è‰²åŠŸèƒ½
            - ğŸ¯ **YOLO11æ£€æµ‹**: å¿«é€Ÿå‡†ç¡®çš„ç›®æ ‡æ£€æµ‹
            - ğŸ¨ **SAM2åˆ†å‰²**: åƒç´ çº§ç²¾ç¡®åˆ†å‰²
            - ğŸ¬ **è§†é¢‘å¤„ç†**: æ”¯æŒè§†é¢‘åºåˆ—åˆ†å‰²
            - ğŸ“Š **å®æ—¶é¢„è§ˆ**: å³æ—¶æŸ¥çœ‹å¤„ç†ç»“æœ
            - ğŸ’¾ **å¤šç§å¯¼å‡º**: æ©ç ã€è§†é¢‘ç­‰æ ¼å¼
            """)

    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    if st.sidebar.button("ğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶"):
        # æ¸…ç†ä¸´æ—¶è§†é¢‘å’Œå¸§ç›®å½•
        for item in os.listdir('.'):
            if item.startswith('temp_video_') or item.startswith('frames_'):
                if os.path.isfile(item):
                    os.remove(item)
                elif os.path.isdir(item):
                    shutil.rmtree(item)
        
        # æ¸…ç†session state
        for key in ['video_segments', 'frame_paths', 'detections', 'frame_dir']:
            if key in st.session_state:
                del st.session_state[key]
        
        st.success("âœ… ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†")
        st.rerun()

if __name__ == "__main__":
    main() 